{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "#import fasttext\n",
    "#import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edeaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb71cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052074671668017\n",
      "0.7045793841375075\n",
      "0.8142993898625756\n",
      "0.7122102717678036\n"
     ]
    }
   ],
   "source": [
    "text = SnowNLP(u'口感很好，喝起来味道不错，包装也很精美，送人也很大气。')\n",
    "sent = text.sentences\n",
    "for sen in sent:\n",
    "    s = SnowNLP(sen)\n",
    "    print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['送', '人', '也', '很', '大', '气']\n"
     ]
    }
   ],
   "source": [
    "print(s.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3192e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('送', 'v'), ('人', 'n'), ('也', 'd'), ('很', 'd'), ('大', 'a'), ('气', 'n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab6a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e398a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"teapro.csv\", encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc8ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        no                  rateContent  package  quality  price  service  \\\n",
      "0        1  口感很好，喝起来味道不错，包装也很精美，送人也很大气。        1        1      0        0   \n",
      "1        2  送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！        1        0      0        0   \n",
      "2        3            茶的味道很纯正，使用方便，包装很好        1        1      0        0   \n",
      "3        4              茶叶不错，味道挺好的，5分好评        0        1      0        0   \n",
      "4        5                     口感特别好~~！        0        1      0        0   \n",
      "...    ...                          ...      ...      ...    ...      ...   \n",
      "3842  3843                很好，很新鲜，不错，好评！        0        0      0        0   \n",
      "3843  3844               不错，是正品，下次还会再来。        0        1      0        0   \n",
      "3844  3845      老板态度好，发货及时，茶叶很好，口感很好，甘甜        0        1      0        1   \n",
      "3845  3846                     哎，，，，，，，        0        0      0        0   \n",
      "3846  3847                       味道真的不错        0        1      0        0   \n",
      "\n",
      "      logistics  other  sentiment  \n",
      "0             0      0          0  \n",
      "1             0      0          1  \n",
      "2             0      0          0  \n",
      "3             0      0          0  \n",
      "4             0      0          0  \n",
      "...         ...    ...        ...  \n",
      "3842          0      0          0  \n",
      "3843          0      0          0  \n",
      "3844          1      0          0  \n",
      "3845          0      0          1  \n",
      "3846          0      0          0  \n",
      "\n",
      "[3847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcfb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c98ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n",
      "1       送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！\n",
      "2                 茶的味道很纯正，使用方便，包装很好\n",
      "3                   茶叶不错，味道挺好的，5分好评\n",
      "4                          口感特别好~~！\n",
      "                   ...             \n",
      "3842                  很好，很新鲜，不错，好评！\n",
      "3843                 不错，是正品，下次还会再来。\n",
      "3844        老板态度好，发货及时，茶叶很好，口感很好，甘甜\n",
      "3845                       哎，，，，，，，\n",
      "3846                         味道真的不错\n",
      "Name: rateContent, Length: 3847, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[:,\"rateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2fa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n"
     ]
    }
   ],
   "source": [
    "training_data_list = []\n",
    "print(train.loc[:,\"rateContent\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc74bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[:,\"rateContent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f016d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    training_data_list.append(train.loc[:,\"rateContent\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_data_list.append(train.loc[:,\"rateContent\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    label_list.append(train.loc[:,\"package\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7598d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_label_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_label_list.append(train.loc[:,\"package\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40ddd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36865e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba976637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(training_data_list)\n",
    "val_inputs, val_masks = preprocessing_for_bert(vali_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(label_list)\n",
    "val_labels = torch.tensor(vali_label_list)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e576971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9704e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0618c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba8705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a3a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.440739   |     -      |     -     |   2.43   \n",
      "   1    |   40    |   0.179634   |     -      |     -     |   1.93   \n",
      "   1    |   60    |   0.148921   |     -      |     -     |   1.90   \n",
      "   1    |   80    |   0.136034   |     -      |     -     |   1.90   \n",
      "   1    |   100   |   0.139765   |     -      |     -     |   1.90   \n",
      "   1    |   117   |   0.114921   |     -      |     -     |   1.57   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.197426   |  0.065580  |   96.88   |   11.71  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.138381   |     -      |     -     |   1.99   \n",
      "   2    |   40    |   0.117200   |     -      |     -     |   1.90   \n",
      "   2    |   60    |   0.132354   |     -      |     -     |   1.91   \n",
      "   2    |   80    |   0.117249   |     -      |     -     |   1.91   \n",
      "   2    |   100   |   0.150837   |     -      |     -     |   1.91   \n",
      "   2    |   117   |   0.116918   |     -      |     -     |   1.59   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.129207   |  0.081722  |   97.66   |   11.31  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "865099b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fb214cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07375bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a05f82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9902\n",
      "Accuracy: 97.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyIUlEQVR4nO3dd5xU5fXH8c8BKYqACsYYmqioFGkSEBtYUEQQDYhoLNjQ2LD+xBhjidEYjLEECyhihSgqYoUoIKLSQaoogsCiWBAVpMjC+f3x3HWHdXd22N0pO/t9v17z2rll7j1zd3fO3Oe59zzm7oiIiBSlUroDEBGRzKZEISIicSlRiIhIXEoUIiISlxKFiIjEpUQhIiJxKVHIDjGzBWbWOd1xZAoz+7OZPZamfQ83szvSse+yZmZ/NLNxJXyt/iaTTImiHDOzz81so5mtN7PV0QfHrsncp7s3d/eJydxHHjOrZmZ3mdmK6H1+ambXm5mlYv+FxNPZzHJi57n7ne5+YZL2Z2Z2pZnNN7OfzCzHzF4ws4OTsb+SMrNbzeyZ0mzD3Z919+MT2NevkmMq/yYrKiWK8q+Hu+8KtAbaADemN5wdZ2Y7FbHoBeBYoBtQEzgb6A/cn4QYzMwy7f/hfmAAcCWwB3AAMBo4qax3FOd3kHTp3LckyN31KKcP4HPguJjpfwKvx0wfCnwAfA98BHSOWbYH8ATwBbAWGB2zrDswJ3rdB0DLgvsEfgdsBPaIWdYG+BaoEk2fDyyKtj8WaBSzrgOXAZ8Cywp5b8cCm4AGBeZ3ALYC+0fTE4G7gGnAj8ArBWKKdwwmAn8H3o/ey/7AeVHM64ClwMXRujWidbYB66PH74BbgWeidfaJ3te5wIroWNwUs7+dgSej47EI+D8gp4jfbZPofbaP8/sfDgwGXo/inQrsF7P8fmBldFxmAkfGLLsVGAU8Ey2/EGgPfBgdqy+B/wBVY17THPgf8B3wFfBnoCvwM7AlOiYfRevWBh6PtrMKuAOoHC3rFx3zfwNromX9gMnRcouWfR3FNg9oQfiSsCXa33rg1YL/B0DlKK7PomMykwJ/Q3qU4LMm3QHoUYpf3vb/IPWjf6j7o+l60T9hN8KZY5does9o+evAf4HdgSpAp2h+m+gftEP0T3dutJ9qhexzPHBRTDyDgEei5z2BJUBTYCfgL8AHMet69KGzB7BzIe/tH8C7Rbzv5eR/gE+MPohaED7MXyT/g7u4YzCR8IHePIqxCuHb+n7Rh1UnYAPQNlq/MwU+2Ck8UQwlJIVWwGagaex7io55fWBuwe3FbPcSYHkxv//h0ftpH8X/LDAyZvlZQJ1o2bXAaqB6TNxbgFOiY7MzcAghse4UvZdFwFXR+jUJH/rXAtWj6Q4Fj0HMvl8GHo1+J78hJPK831k/IBe4ItrXzmyfKE4gfMDvFv0emgJ7x7znO+L8H1xP+D84MHptK6BOuv9Xy/sj7QHoUYpfXvgHWU/45uTAO8Bu0bIbgKcLrD+W8MG/N+Gb8e6FbPNh4G8F5i0mP5HE/lNeCIyPnhvh2+tR0fSbwAUx26hE+NBtFE07cEyc9/ZY7IdegWVTiL6pEz7s/xGzrBnhG2fleMcg5rW3F3OMRwMDouedSSxR1I9ZPg3oGz1fCpwQs+zCgtuLWXYTMKWY2IYDj8VMdwM+jrP+WqBVTNyTitn+VcDL0fMzgNlFrPfLMYim9yIkyJ1j5p0BTIie9wNWFNhGP/ITxTHAJ4SkVamQ9xwvUSwGepb2f0uP7R+Z1iYrO+4Ud69J+BA7CKgbzW8EnGZm3+c9gCMISaIB8J27ry1ke42Aawu8rgGhmaWgF4GOZrY3cBQh+bwXs537Y7bxHSGZ1It5/co47+vbKNbC7B0tL2w7ywlnBnWJfwwKjcHMTjSzKWb2XbR+N/KPaaJWxzzfAORdYPC7AvuL9/7XUPT7T2RfmNl1ZrbIzH6I3ktttn8vBd/7AWb2WnRhxI/AnTHrNyA05ySiEeF38GXMcX+UcGZR6L5juft4QrPXYOBrMxtiZrUS3PeOxCkJUqLIEu7+LuHb1j3RrJWEb9O7xTxquPs/omV7mNluhWxqJfD3Aq/bxd1HFLLPtcA44HTgTMIZgMds5+IC29nZ3T+I3USct/Q20MHMGsTONLMOhA+D8TGzY9dpSGhS+baYY/CrGMysGiH53QPs5e67AW8QElxx8SbiS0KTU2FxF/QOUN/M2pVkR2Z2JKEPpA/hzHE34Afy3wv8+v08DHwMNHH3WoS2/rz1VwL7FrG7gttZSTijqBtz3Gu5e/M4r9l+g+4PuPshhDPEAwhNSsW+Ltr3fsWsIztIiSK73Ad0MbNWhE7KHmZ2gplVNrPq0eWd9d39S0LT0ENmtruZVTGzo6JtDAUuMbMO0ZVANczsJDOrWcQ+nwPOAXpHz/M8AtxoZs0BzKy2mZ2W6Btx97cJH5Yvmlnz6D0cGr2vh93905jVzzKzZma2C3A7MMrdt8Y7BkXstipQDfgGyDWzE4HYSza/AuqYWe1E30cBzxOOye5mVg+4vKgVo/f3EDAiirlqFH9fMxuYwL5qEvoBvgF2MrO/AsV9K69J6Dxeb2YHAX+KWfYasLeZXRVdtlwzStoQjss+eVeNRX9f44B/mVktM6tkZvuZWacE4sbMfh/9/VUBfiJc1LAtZl9FJSwITZZ/M7Mm0d9vSzOrk8h+pWhKFFnE3b8BngL+6u4rCR3KfyZ8WKwkfCvL+52fTfjm/TGh8/qqaBszgIsIp/5rCR3S/eLsdgzhCp3V7v5RTCwvA3cDI6NmjPnAiTv4lnoBE4C3CH0xzxCupLmiwHpPE86mVhM6Wq+MYijuGGzH3ddFr32e8N7PjN5f3vKPgRHA0qhJpbDmuHhuB3KAZYQzplGEb95FuZL8JpjvCU0qpwKvJrCvsYTj9gmhOW4T8Zu6AK4jvOd1hC8M/81bEB2bLkAPwnH+FDg6WvxC9HONmc2Knp9DSLwLCcdyFIk1pUFIaEOj1y0nNMMNipY9DjSLjv/oQl57L+H3N46Q9B4ndJZLKVh+S4FI+WNmEwkdqWm5O7o0zOxPhI7uhL5pi6SLzihEUsTM9jazw6OmmAMJl5q+nO64RIqTtERhZsPM7Gszm1/EcjOzB8xsiZnNNbO2yYpFJENUJVz9s47QGf8KoR9CJKMlrekp6hxdDzzl7i0KWd6N0NbcjXBz1/3u3qHgeiIikl5JO6Nw90mEa+eL0pOQRNzdpwC7Rdfji4hIBklnMa56bH8VRk4078uCK5pZf0KdF2rUqHHIQQcdtMM7W7wYNm6EnXX9g4hUIHttXs6uud/zked+6+57lmQb5aJqo7sPAYYAtGvXzmfMmLHD2+jcOfycOLHs4hIRyUh5XQpm8PDD8PXX2K23Li/p5tJ51dMqtr8ztX40T0RESmrVKujZE56L7n/905/glltKtcl0JooxwDnR1U+HAj9Ed3SKiMiOcoehQ6FZM3j7bVi/vsw2nbSmJzMbQShUV9fCqGC3EAqF4e6PEGrodCPc+buBMA6AiIjsqM8+g4suggkT4OijQ8LYr+xKXiUtUbj7GcUsd8LANSIiUhrz5sHMmTBkCFx4YeibKEPlojNbREQKmD8fZs2Cc86BU06BpUuhTnLqH6qEh4hIefLzz3DrrdC2Ldx0E2zaFOYnKUmAEoWISPkxdWpIELfdBqefDrNnQ/XqSd+tmp5ERMqDVavgyCNhr73gtdfgpJNStmudUYiIZLJPPgk/69WD//4XFixIaZIAJQoRkcz0/ffQvz8cdBBMmhTmnXoq1Ep0+PCyo6YnEZFMM2ZMuKN69Wq4/nr4/e/TGo4ShYhIJrnwQnj8cTj4YHjlFWjXLt0RKVGIiKRdbBG/du2gUSO44QaoWjW9cUWUKERE0mnlSrjkEujbF84+OzzPMOrMFhFJh23bQgnw5s3D+AebN6c7oiLpjEJEJNU+/TT0RUyaBMcdF2o0NW6c7qiKpEQhIpJqCxfC3LkwbBj061fmRfzKmhKFiEgqfPQRzJkD554bBhZauhR23z3dUSVEfRQiIsm0eTPcfHO4munmm/OL+JWTJAFKFCIiyfPhh9CmDdxxB5x5ZsqK+JU1NT2JiCTDqlXQqRP89rfwxhtw4onpjqjEdEYhIlKWFi0KP+vVg+efD0X8ynGSgCw6oxgyBJ57rujlc+ZA69apikZEKpy1a+Haa+GJJ8Jlr0ceGUaeywJZc0bx3HMhGRSldevQRCgiUuZefhmaNYOnnoIbb0x7Eb+yljVnFBCSwcSJ6Y5CRCqU888PZxGtW8Prr4cR6LJMViUKEZGUiC3id+ih0KQJXHcdVKmS3riSRIlCRGRHLF8OF18c2rLPOScMLpTlsqaPQkQkqbZtg8GDoUULmDwZtmxJd0QpozMKEZHiLF4civhNngzHHw+PPgr77JPuqFJGiUJEpDiLF4f7IYYPD81NGV7Er6wpUYiIFGb27HDN/XnnwcknhyJ+u+2W7qjSQn0UIiKxNm2CP/853Atx6635RfwqaJIAJQoRkXzvvx/uh7jrrtDENGdOuSziV9bU9CQiAqGI39FHhxpNY8eGTmsBdEYhIhXdwoXhZ7168OKLMG+ekkQBShQiUjF9910YhrR581DED6BHD9h117SGlYnU9CQiFc+LL8Jll8GaNXDTTdC+fbojymhKFCJSsfTrB08+GYr3vfWWxh9IgBKFiGS/2CJ+hx0GTZuGsSN20kdgIpLaR2FmXc1ssZktMbOBhSxvaGYTzGy2mc01s27JjEdEKqBly0Ln9FNPhen+/eGGG5QkdkDSEoWZVQYGAycCzYAzzKxZgdX+Ajzv7m2AvsBDyYpHRCqYrVvhgQdCEb8pU/LPKmSHJfOMoj2wxN2XuvvPwEigZ4F1HKgVPa8NfJHEeESkoli0KAxFOmAAdOoU6jT165fuqMqtZJ571QNWxkznAB0KrHMrMM7MrgBqAMcVtiEz6w/0B2jYsGGZByoiWWbJklDI7+mn4Y9/rHBF/Mpauu+jOAMY7u71gW7A02b2q5jcfYi7t3P3dnvuuWfKgxSRcmDmTBg2LDzv0SP0TZx1lpJEGUhmolgFNIiZrh/Ni3UB8DyAu38IVAfqJjEmEck2GzfCwIHQoQP87W/5Rfxq1Yr/OklYMhPFdKCJmTU2s6qEzuoxBdZZARwLYGZNCYnimyTGJCLZZNIkaNUK7r479EHMnq0ifkmQtD4Kd881s8uBsUBlYJi7LzCz24EZ7j4GuBYYamZXEzq2+7nr0gQRScCqVXDssdCgAbz9dnguSZHUC4nd/Q3gjQLz/hrzfCFweDJjEJEsM28eHHxwKOL38suh4muNGumOKquluzNbRCQx334LZ58NLVvmF/Hr3l1JIgV0a6KIZDZ3eOEFuPxyWLsWbrkldFxLyihRiEhmO/fccD9Eu3bwzjuh2UlSSolCRDJPbBG/Tp1Cc9NVV6k+U5qoj0JEMsvSpXDccTB8eJi+4AK47joliTRSohCRzLB1K9x3X2hamj4dKunjKVMoRYtI+i1cCOefD1OnwkknwSOPQP366Y5KIkoUIpJ+y5bBZ5/Bc89B376qz5RhlChEJD2mT4c5c+Cii8JZxNKlULNmuqOSQqgRUERSa8OG0Dl96KFw1135RfyUJDKWEoWIpM7EieFS13/9K5xJqIhfuaCmJxFJjZwc6NIFGjWC8eNDjSYpF3RGISLJ9dFH4Wf9+vDKKzB3rpJEOaNEISLJ8c03cOaZ0Lo1vPtumNetG+yyS1rDkh2npicRKVvuMHIkXHkl/PAD3HYbdOyY7qikFJQoRKRsnX02PPtsqPD6+OPQvHm6I5JSSjhRmNku7r4hmcGISDm1bVu4Sc4s9D8cckg4o6hcOd2RSRkoto/CzA4zs4XAx9F0KzN7KOmRiUj5sGRJGIb0iSfC9AUXwNVXK0lkkUQ6s/8NnACsAXD3j4CjkhmUiJQDublwzz2hiN/s2VC1arojkiRJqOnJ3Vfa9rVXtiYnHBEpF+bPh/POgxkzoGdPeOgh+N3v0h2VJEkiiWKlmR0GuJlVAQYAi5IblohktBUrYPnycHVTnz4q4pflEkkUlwD3A/WAVcA44NJkBiUiGWjq1HDzXP/+4X6IpUth113THZWkQCKJ4kB3/2PsDDM7HHg/OSHFt3gxdO786/lz5oT7ekSkjP30E9x8cxhUaN99wxjW1aopSVQgiXRmP5jgvJTYuLHw+a1bh5tARaQMjR8fivj9+99wySUwa1ZIElKhFHlGYWYdgcOAPc3smphFtYC0Xfe2886hAKWIJFlODpxwAjRuHEpwHKWLHSuqeE1PVYFdo3ViC8X/CPROZlAikkazZ0ObNqGI36uvQqdO4RuaVFjm7vFXMGvk7stTFE+xatZs5+vWzUh3GCLZ56uvwt3Uzz8fTts7dUp3RFKGzGymu7cryWsT6czeYGaDgObALyOMuPsxJdmhiGQY91CbacAAWL8e7rgDDjss3VFJBkmkM/tZQvmOxsBtwOfA9CTGJCKpdOaZoZDfgQeGywdvugmqVEl3VJJBEjmjqOPuj5vZAHd/F3jXzJQoRMqz2CJ+xx8fyoBfdpnqM0mhEjmj2BL9/NLMTjKzNsAeSYxJRJLpk09Chddhw8L0eeep0qvElcgZxR1mVhu4lnD/RC3gqmQGJSJJkJsL994Lt9wC1avrSiZJWLGJwt1fi57+ABwNv9yZLSLlxdy5cP75MHMmnHoqDB4Me++d7qiknIh3w11loA+hxtNb7j7fzLoDfwZ2BtqkJkQRKbWcHFi5El54AXr1UhE/2SHx+igeBy4E6gAPmNkzwD3AP909oSRhZl3NbLGZLTGzgUWs08fMFprZAjN7bkffgIgU4YMP4JFHwvO8In69eytJyA6L1/TUDmjp7tvMrDqwGtjP3dcksuHojGQw0AXIAaab2Rh3XxizThPgRuBwd19rZr8p6RsRkcj69eES1wcfhP32C53V1apBjRrpjkzKqXhnFD+7+zYAd98ELE00SUTaA0vcfam7/wyMBHoWWOciYLC7r4328/UObF9ECho3Dlq0CEnisstUxE/KRLwzioPMbG703ID9omkD3N1bFrPtesDKmOkcoEOBdQ4AMLP3CYUGb3X3twpuyMz6A/0BqlUrbrciFdTKlXDSSeEsYtIkOOKIdEckWSJeomiaov03AToD9YFJZnawu38fu5K7DwGGQKj1lIK4RMqPmTPhkEOgQQN44w048shw+atIGSmy6cndl8d7JLDtVUCDmOn60bxYOcAYd9/i7suATwiJQ0SKs3o1nHYatGsXyoADdOmiJCFlLpE7s0tqOtDEzBqbWVWgLzCmwDqjCWcTmFldQlPU0iTGJFL+ucOTT0KzZqEM+J13qoifJFUid2aXiLvnmtnlwFhC/8Mwd19gZrcDM9x9TLTseDNbCGwFrt/BDnORiqdv31AK/PDD4bHH4KCD0h2RZLlix6MAMLOdgYbuvjj5IcWn8SikQoot4vfkk7BuHVx6KVRKZqOAZJPSjEdR7F+ZmfUA5gBvRdOtzaxgE5KIJMvHH4dhSB9/PEyfey5cfrmShKRMIn9ptxLuifgewN3nEMamEJFk2rIl9D+0agULF8Kuu6Y7IqmgEumj2OLuP9j2t/3rElWRZJozJ9xRPWdOKLvx4IPw29+mOyqpoBJJFAvM7EygclRy40rgg+SGJVLBrV4dHi++CH/4Q7qjkQoukaanKwjjZW8GniOUG78qiTGJVEyTJ8NDD4XnXbvCZ58pSUhGKPaqJzNr6+6zUhRPsXTVk2SddevgxhvDGBFNmsC8earPJGUuqVc9Af8ys0Vm9jcza1GSnYhIEcaODUX8HnoIBgxQET/JSMUmCnc/mjCy3TfAo2Y2z8z+kvTIRLLdypXQvTvssktodrrvPl3ZJBkpoQux3X21uz8AXEK4p+KvyQxKJGu5w7Rp4XmDBvDmmzB7tkpwSEZL5Ia7pmZ2q5nNAx4kXPFUP+mRiWSbL78Mw5B26JBfxO+441TETzJeIpfHDgP+C5zg7l8kOR6R7OMOw4fDNdfApk1w992hTpNIOVFsonD3jqkIRCRr9ekDo0aFcSIeewwOOCDdEYnskCIThZk97+59oian2GtoEx3hTqTi2ro1FPCrVAl69IBjjoGLL1Z9JimX4p1RDIh+dk9FICJZY9EiuOCCUILjoovgnHPSHZFIqcQb4e7L6OmlhYxud2lqwhMpR7ZsgTvugNatYfFiqF073RGJlIlEzoO7FDLvxLIORKRcmz07DEl6881w6qnhrKJPn3RHJVIm4vVR/Ilw5rCvmc2NWVQTeD/ZgYmUK199Bd9+C6NHQ8+e6Y5GpEwVWevJzGoDuwN3AQNjFq1z9+9SEFuhVOtJMsakSaEu02WXhemNG2HnndMbk0gRklXryd39c+AyYF3MAzPboyQ7E8kKP/4YhiHt1AkeeAA2bw7zlSQkS8W76uk5whVPMwmXx8aOXOTAvkmMSyQzvfFGuMz1iy/CDXS3364ifpL1ikwU7t49+qlhT0UgFPHr2RMOPDDcQNehQ7ojEkmJRGo9HW5mNaLnZ5nZvWbWMPmhiWQAd5gyJTxv0ADGjQulwJUkpAJJ5PLYh4ENZtYKuBb4DHg6qVGJZIIvvoBTToGOHfOL+B19NFStmtawRFItkUSR6+HSqJ7Af9x9MOESWZHs5B5qMjVrFs4g7rlHRfykQkukeuw6M7sROBs40swqAVWSG5ZIGvXuDS+9FK5qeuwx2H//dEckklaJnFGcDmwGznf31YSxKAYlNSqRVNu6FbZtC89POQUeeQTGj1eSECHODXfbrWS2F/D7aHKau3+d1Kji0A13Uubmz4cLLwyF/C66KN3RiCRFsm64y9t4H2AacBrQB5hqZr1LsjORjPLzz3DbbdC2LXz2Gey+e7ojEslIifRR3AT8Pu8swsz2BN4GRiUzMJGkmjkT+vULZxNnngn33Qd77pnuqEQyUiKJolKBpqY1JNa3IZK51qyB77+HV1+F7hpyRSSeRBLFW2Y2FhgRTZ8OvJG8kESSZMKEUMTvyivh+OPh00+hevV0RyWS8Yo9M3D364FHgZbRY4i735DswETKzA8/hPpMxxwDDz+cX8RPSUIkIfHGo2gC3APsB8wDrnP3VakKTKRMvPoqXHIJrF4N110XOq9VxE9kh8Q7oxgGvAb0IlSQfTAlEYmUlZUroVcvqFMn1GsaNAh22SXdUYmUO/H6KGq6+9Do+WIzm5WKgERKxR0+/BAOOyy/iN9hh6k+k0gpxDujqG5mbcysrZm1BXYuMF0sM+tqZovNbImZDYyzXi8zczMr0c0gIgDk5MDJJ4e6THlF/Dp3VpIQKaV4ZxRfAvfGTK+OmXbgmHgbNrPKwGCgC5ADTDezMe6+sMB6NYEBwNQdC10ksm0bDB0K118Publw771wxBHpjkoka8QbuOjoUm67PbDE3ZcCmNlIQgXahQXW+xtwN3B9KfcnFVWvXjB6dLiqaehQ2FeDL4qUpWTeOFcPWBkznRPN+0XUhNXA3V+PtyEz629mM8xsxpYtW8o+Uil/cnPzi/j16hUSxNtvK0mIJEHa7rCOypXfSxgMKS53H+Lu7dy9XZUqqnBe4c2dGwYTGhpda3HWWaGon1n814lIiSQzUawCGsRM14/m5akJtAAmmtnnwKHAGHVoS5E2b4ZbboFDDoHly1WbSSRFEqkea9FY2X+NphuaWfsEtj0daGJmjc2sKtAXGJO30N1/cPe67r6Pu+8DTAFOdnfVEJdfmz49VHm9/XY44wxYtAj+8Id0RyVSISRyRvEQ0BE4I5peR7iaKS53zwUuB8YCi4Dn3X2Bmd1uZieXMF6pqNauhfXr4Y034Kmnwk10IpISxQ5cZGaz3L2tmc129zbRvI/cvVVKIixAAxdVIOPHhyJ+AwaE6c2bVX5DpISSOnARsCW6J8Kjne0JbCvJzkQS8v33YaS5Y4+FRx/NL+KnJCGSFokkigeAl4HfmNnfgcnAnUmNSiquV16BZs1g2DD4v/8LAwwpQYikVbHjUbj7s2Y2EzgWMOAUd1+U9Mik4lmxAk47DZo2hTFjoJ0ugBPJBMUmCjNrCGwAXo2d5+4rkhmYVBDuMHkyHHkkNGwYbpo79FDVZxLJIImMcPc6oX/CgOpAY2Ax0DyJcUlFsGJFGCvizTdh4kTo1AmOOirdUYlIAYk0PR0cOx2V3bg0aRFJ9tu2DR55BG64IZxRPPCAiviJZLBEzii24+6zzKxDMoKRCuIPfwid1l26wJAhsM8+6Y5IROJIpI/impjJSkBb4IukRSTZKTcXKlUKj9NPh549oV8/1WcSKQcSuTy2ZsyjGqHPomcyg5Is89FH0KFDOHuAUILjvPOUJETKibhnFNGNdjXd/boUxSPZZNMmuOMOuPtu2GMP+O1v0x2RiJRAkYnCzHZy91wzOzyVAUmWmDYNzj0XPv44/Lz33pAsRKTciXdGMY3QHzHHzMYALwA/5S1095eSHJuUZz/+CBs3wltvwQknpDsaESmFRK56qg6sIYyRnXc/hQNKFLK9ceNgwQK4+mo47jhYvFjlN0SyQLxE8Zvoiqf55CeIPPFLzkrFsnYtXHMNDB8OzZvDpZeGBKEkIZIV4l31VBnYNXrUjHme9xCBl14KRfyefhpuvBFmzFCCEMky8c4ovnT321MWiZQ/K1ZA377QokUYUKhNm3RHJCJJEO+MQhe5y6+5w7vvhucNG4bBhaZOVZIQyWLxEsWxKYtCyofly+HEE6Fz5/xkccQRUKVKWsMSkeQqMlG4+3epDEQy2LZt8J//hI7qyZPhwQdDWXARqRB2uCigVECnnAKvvhruh3j0UWjUKN0RiUgKKVFI4bZsgcqVQxG/M86A3r3h7LNVn0mkAkqkKKBUNLNmQfv2YcwICIninHOUJEQqKCUKybdxY7gXon17WL0aGjRId0QikgHU9CTBlCmheN8nn8D558M998Duu6c7KhHJAEoUEvz0U+iX+N//Qp0mEZGIEkVF9tZboYjftdfCsceGkuBVq6Y7KhHJMOqjqIjWrAnNTCeeCE8+CT//HOYrSYhIIZQoKhJ3GDUqFPF77jn4y19g+nQlCBGJS01PFcmKFXDmmdCyZRg7olWrdEckIuWAziiynXso3AfhjuqJE8MVTkoSIpIgJYpstmwZHH986KjOK+J32GGwk04kRSRxShTZaOtWuP/+ME7E1Knw8MMq4iciJaavltmoZ094/XXo1i2U4dAd1iJSCkoU2SK2iN/ZZ4f6TGeeqfpMIlJqSW16MrOuZrbYzJaY2cBCll9jZgvNbK6ZvWNmql9dEjNmQLt2oYkJ4PTT4Y9/VJIQkTKRtERhZpWBwcCJQDPgDDNrVmC12UA7d28JjAL+max4stLGjXDDDdChA3zzjcaJEJGkSOYZRXtgibsvdfefgZFAz9gV3H2Cu2+IJqcA9ZMYT3b58MNwies//xmK+C1cCN27pzsqEclCyeyjqAesjJnOATrEWf8C4M3CFphZf6A/QLVqLcsqvvJt48YwROnbb4fLX0VEkiQjOrPN7CygHdCpsOXuPgQYAlCzZjtPYWiZ5Y03QhG/66+HY46BRYugSpV0RyUiWS6ZTU+rgNjrMutH87ZjZscBNwEnu/vmJMZTfn37LZx1Fpx0Ejz7bH4RPyUJEUmBZCaK6UATM2tsZlWBvsCY2BXMrA3wKCFJfJ3EWMondxg5Epo2heefh1tugWnTVMRPRFIqaU1P7p5rZpcDY4HKwDB3X2BmtwMz3H0MMAjYFXjBwqWcK9z95GTFVO6sWBHKgbdqBY8/DgcfnO6IRKQCMvfy1eRfs2Y7X7duRrrDSB53eOed/FHmpkyB3/8+3EwnIlJCZjbT3duV5LWq9ZRJPvssXMHUpUt+Eb9DD1WSEJG0UqLIBFu3wr33hqalmTPh0UdVxE9EMkZGXB5b4fXoAW++GW6Ye/hhqK/7DkUkcyhRpMvPP4dxISpVgn79QiG/vn1Vn0lEMo6antJh2jQ45BB46KEw3adPqPaqJCEiGUiJIpU2bIBrr4WOHWHtWthvv3RHJCJSLDU9pcrkyeGeiKVL4eKL4e67oXbtdEclIlIsJYpUyRtYaMIE6Nw53dGIiCRMiSKZXn01FO77v/+Do48OpcB30iEXkfJFfRTJ8M03YRjSk0+GESPyi/gpSYhIOaREUZbc4bnnQhG/UaPg9tth6lQV8RORck1fccvSihVw3nnQpk0o4te8ebojEhEpNZ1RlNa2bTB2bHjeqBG89x68/76ShIhkDSWK0vj00zDSXNeuMGlSmNe+vYr4iUhWUaIoidxcGDQIWraEOXNCM5OK+IlIllIfRUl07x6am3r2DGU4fve7dEckkpG2bNlCTk4OmzZtSncoFUb16tWpX78+VcpwqGQNXJSozZvDGNWVKoUrmrZtg9NOU30mkTiWLVtGzZo1qVOnDqb/laRzd9asWcO6deto3Ljxdss0cFGyTZkCbdvC4MFhunfvUMhPf/gicW3atElJIoXMjDp16pT5GZwSRTw//QRXXw2HHQbr1kGTJumOSKTcUZJIrWQcb/VRFOW990IRv2XL4NJL4a67oFatdEclIpJyOqMoSm5u6JN4993Q5KQkIVJujR49GjPj448//mXexIkT6d69+3br9evXj1GjRgGhI37gwIE0adKEtm3b0rFjR958881Sx3LXXXex//77c+CBBzI27x6sAsaPH0/btm1p0aIF5557Lrm5uQCsXbuWU089lZYtW9K+fXvmz59f6ngSoUQRa/TocOYAoYjfggVw1FFpDUlESm/EiBEcccQRjBgxIuHX3HzzzXz55ZfMnz+fWbNmMXr0aNatW1eqOBYuXMjIkSNZsGABb731Fpdeeilbt27dbp1t27Zx7rnnMnLkSObPn0+jRo148sknAbjzzjtp3bo1c+fO5amnnmLAgAGliidRanoC+OoruOIKeOGF0Gl97bWhPpOK+ImUmauuCrcdlaXWreG+++Kvs379eiZPnsyECRPo0aMHt912W7Hb3bBhA0OHDmXZsmVUq1YNgL322os+ffqUKt5XXnmFvn37Uq1aNRo3bsz+++/PtGnT6Nix4y/rrFmzhqpVq3LAAQcA0KVLF+666y4uuOACFi5cyMCBAwE46KCD+Pzzz/nqq6/Ya6+9ShVXcSr2GYU7PP00NGsGr7wCf/97uMJJRfxEssYrr7xC165dOeCAA6hTpw4zZ84s9jVLliyhYcOG1Eqgyfnqq6+mdevWv3r84x//+NW6q1atokGDBr9M169fn1WrVm23Tt26dcnNzWXGjHAbwKhRo1i5ciUArVq14qWXXgJg2rRpLF++nJycnGJjLK2K/ZV5xQq48EJo1y7cXX3QQemOSCRrFffNP1lGjBjxSxNN3759GTFiBIccckiRVwft6FVD//73v0sdY8H9jxw5kquvvprNmzdz/PHHUzkqCzRw4EAGDBhA69atOfjgg2nTps0vy5Kp4iWKvCJ+J54Yivi9/36o9qr6TCJZ57vvvmP8+PHMmzcPM2Pr1q2YGYMGDaJOnTqsXbv2V+vXrVuX/fffnxUrVvDjjz8We1Zx9dVXM2HChF/N79u37y/NRHnq1av3y9kBQE5ODvXq1fvVazt27Mh7770HwLhx4/jkk08AqFWrFk888QQQbq5r3Lgx++67bwJHopTcvVw9dt31EC+xxYvdjzzSHdwnTiz5dkQkIQsXLkzr/h999FHv37//dvOOOuoof/fdd33Tpk2+zz77/BLj559/7g0bNvTvv//e3d2vv/5679evn2/evNnd3b/++mt//vnnSxXP/PnzvWXLlr5p0yZfunSpN27c2HNzc3+13ldffeXu7ps2bfJjjjnG33nnHXd3X7t27S/xDBkyxM8+++xC91PYcQdmeAk/dytGH0VuLtx9dyjiN28ePPGErmYSqQBGjBjBqaeeut28Xr16MWLECKpVq8YzzzzDeeedR+vWrenduzePPfYYtWvXBuCOO+5gzz33pFmzZrRo0YLu3bsn1GcRT/PmzenTpw/NmjWja9euDB48+Jemo27duvHFF18AMGjQIJo2bUrLli3p0aMHxxxzDACLFi2iRYsWHHjggbz55pvcf//9pYonURWj1tMJJ8C4cfCHP4R7In772+QEJyLbWbRoEU2bNk13GBVOYce9NLWesrePYtOmcMNc5crQv3949OqV7qhERMqd7Gx6ev/9cIF1XhG/Xr2UJERESii7EsX69XDllWEQoU2bQKe8ImlX3pq3y7tkHO/sSRTvvgstWsB//gOXXw7z50OXLumOSqRCq169OmvWrFGySBGPxqOoXr16mW43u/oodtklVH09/PB0RyIihDuPc3Jy+Oabb9IdSoWRN8JdWSrfVz299BJ8/DH8+c9heutW3TgnIlKIjB3hzsy6mtliM1tiZgMLWV7NzP4bLZ9qZvsktOHVq8Moc716wcsvw88/h/lKEiIiZS5picLMKgODgROBZsAZZtaswGoXAGvdfX/g38DdxW239pY1oZP6tddCSfAPPlARPxGRJErmGUV7YIm7L3X3n4GRQM8C6/QEnoyejwKOtWIqcu21eXnotP7oIxg4MNwrISIiSZPMzux6wMqY6RygQ1HruHuumf0A1AG+jV3JzPoD/aPJzTZ58nxVegWgLgWOVQWmY5FPxyKfjkW+A0v6wnJx1ZO7DwGGAJjZjJJ2yGQbHYt8Ohb5dCzy6VjkM7MdrH2UL5lNT6uABjHT9aN5ha5jZjsBtYE1SYxJRER2UDITxXSgiZk1NrOqQF9gTIF1xgDnRs97A+O9vF2vKyKS5ZLW9BT1OVwOjAUqA8PcfYGZ3U6oiz4GeBx42syWAN8RkklxhiQr5nJIxyKfjkU+HYt8Ohb5Snwsyt0NdyIiklrZU+tJRESSQolCRETiythEkbTyH+VQAsfiGjNbaGZzzewdM2uUjjhTobhjEbNeLzNzM8vaSyMTORZm1if621hgZs+lOsZUSeB/pKGZTTCz2dH/Sbd0xJlsZjbMzL42s/lFLDczeyA6TnPNrG1CGy7pYNvJfBA6vz8D9gWqAh8BzQqscynwSPS8L/DfdMedxmNxNLBL9PxPFflYROvVBCYBU4B26Y47jX8XTYDZwO7R9G/SHXcaj8UQ4E/R82bA5+mOO0nH4iigLTC/iOXdgDcBAw4Fpiay3Uw9o0hK+Y9yqthj4e4T3H1DNDmFcM9KNkrk7wLgb4S6YZtSGVyKJXIsLgIGu/taAHf/OsUxpkoix8KBWtHz2sAXKYwvZdx9EuEK0qL0BJ7yYAqwm5ntXdx2MzVRFFb+o15R67h7LpBX/iPbJHIsYl1A+MaQjYo9FtGpdAN3fz2VgaVBIn8XBwAHmNn7ZjbFzLqmLLrUSuRY3AqcZWY5wBvAFakJLePs6OcJUE5KeEhizOwsoB3QKd2xpIOZVQLuBfqlOZRMsROh+akz4Sxzkpkd7O7fpzOoNDkDGO7u/zKzjoT7t1q4+7Z0B1YeZOoZhcp/5EvkWGBmxwE3ASe7++YUxZZqxR2LmkALYKKZfU5ogx2TpR3aifxd5ABj3H2Luy8DPiEkjmyTyLG4AHgewN0/BKoTCgZWNAl9nhSUqYlC5T/yFXsszKwN8CghSWRrOzQUcyzc/Qd3r+vu+7j7PoT+mpPdvcTF0DJYIv8jowlnE5hZXUJT1NIUxpgqiRyLFcCxAGbWlJAoKuL4rGOAc6Krnw4FfnD3L4t7UUY2PXnyyn+UOwkei0HArsALUX/+Cnc/OW1BJ0mCx6JCSPBYjAWON7OFwFbgenfPurPuBI/FtcBQM7ua0LHdLxu/WJrZCMKXg7pRf8wtQBUAd3+E0D/TDVgCbADOS2i7WXisRESkDGVq05OIiGQIJQoREYlLiUJEROJSohARkbiUKEREJC4lCslIZrbVzObEPPaJs+76MtjfcDNbFu1rVnT37o5u4zEzaxY9/3OBZR+UNsZoO3nHZb6ZvWpmuxWzfutsrZQqqaPLYyUjmdl6d9+1rNeNs43hwGvuPsrMjgfucfeWpdheqWMqbrtm9iTwibv/Pc76/QgVdC8v61ik4tAZhZQLZrZrNNbGLDObZ2a/qhprZnub2aSYb9xHRvOPN7MPo9e+YGbFfYBPAvaPXntNtK35ZnZVNK+Gmb1uZh9F80+P5k80s3Zm9g9g5yiOZ6Nl66OfI83spJiYh5tZbzOrbGaDzGx6NE7AxQkclg+JCrqZWfvoPc42sw/M7MDoLuXbgdOjWE6PYh9mZtOidQurviuyvXTXT9dDj8IehDuJ50SPlwlVBGpFy+oS7izNOyNeH/28Frgpel6ZUPupLuGDv0Y0/wbgr4XsbzjQO3p+GjAVOASYB9Qg3Pm+AGgD9AKGxry2dvRzItH4F3kxxayTF+OpwJPR86qESp47A/2Bv0TzqwEzgMaFxLk+5v29AHSNpmsBO0XPjwNejJ73A/4T8/o7gbOi57sR6j/VSPfvW4/MfmRkCQ8RYKO7t86bMLMqwJ1mdhSwjfBNei9gdcxrpgPDonVHu/scM+tEGKjm/ai8SVXCN/HCDDKzvxBqAF1AqA30srv/FMXwEnAk8BbwLzO7m9Bc9d4OvK83gfvNrBrQFZjk7huj5q6WZtY7Wq82oYDfsgKv39nM5kTvfxHwv5j1nzSzJoQSFVWK2P/xwMlmdl00XR1oGG1LpFBKFFJe/BHYEzjE3bdYqA5bPXYFd58UJZKTgOFmdi+wFvifu5+RwD6ud/dReRNmdmxhK7n7JxbGvegG3GFm77j77Ym8CXffZGYTgROA0wmD7EAYcewKdx9bzCY2untrM9uFUNvoMuABwmBNE9z91Kjjf2IRrzegl7svTiReEVAfhZQftYGvoyRxNPCrccEtjBX+lbsPBR4jDAk5BTjczPL6HGqY2QEJ7vM94BQz28XMahCajd4zs98BG9z9GUJBxsLGHd4SndkU5r+EYmx5ZycQPvT/lPcaMzsg2mehPIxoeCVwreWX2c8rF90vZtV1hCa4PGOBKyw6vbJQeVgkLiUKKS+eBdqZ2TzgHODjQtbpDHxkZrMJ39bvd/dvCB+cI8xsLqHZ6aBEdujuswh9F9MIfRaPufts4GBgWtQEdAtwRyEvHwLMzevMLmAcYXCptz0M3QkhsS0EZpnZfELZ+Lhn/FEscwmD8vwTuCt677GvmwA0y+vMJpx5VIliWxBNi8Sly2NFRCQunVGIiEhcShQiIhKXEoWIiMSlRCEiInEpUYiISFxKFCIiEpcShYiIxPX/BWoPZdldxLYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3e363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
