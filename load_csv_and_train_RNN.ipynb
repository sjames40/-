{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "#import fasttext\n",
    "#import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edeaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb71cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052074671668017\n",
      "0.7045793841375075\n",
      "0.8142993898625756\n",
      "0.7122102717678036\n"
     ]
    }
   ],
   "source": [
    "text = SnowNLP(u'口感很好，喝起来味道不错，包装也很精美，送人也很大气。')\n",
    "sent = text.sentences\n",
    "for sen in sent:\n",
    "    s = SnowNLP(sen)\n",
    "    print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['送', '人', '也', '很', '大', '气']\n"
     ]
    }
   ],
   "source": [
    "print(s.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3192e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('送', 'v'), ('人', 'n'), ('也', 'd'), ('很', 'd'), ('大', 'a'), ('气', 'n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e398a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"teapro.csv\", encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc8ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        no                  rateContent  package  quality  price  service  \\\n",
      "0        1  口感很好，喝起来味道不错，包装也很精美，送人也很大气。        1        1      0        0   \n",
      "1        2  送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！        1        0      0        0   \n",
      "2        3            茶的味道很纯正，使用方便，包装很好        1        1      0        0   \n",
      "3        4              茶叶不错，味道挺好的，5分好评        0        1      0        0   \n",
      "4        5                     口感特别好~~！        0        1      0        0   \n",
      "...    ...                          ...      ...      ...    ...      ...   \n",
      "3842  3843                很好，很新鲜，不错，好评！        0        0      0        0   \n",
      "3843  3844               不错，是正品，下次还会再来。        0        1      0        0   \n",
      "3844  3845      老板态度好，发货及时，茶叶很好，口感很好，甘甜        0        1      0        1   \n",
      "3845  3846                     哎，，，，，，，        0        0      0        0   \n",
      "3846  3847                       味道真的不错        0        1      0        0   \n",
      "\n",
      "      logistics  other  sentiment  \n",
      "0             0      0          0  \n",
      "1             0      0          1  \n",
      "2             0      0          0  \n",
      "3             0      0          0  \n",
      "4             0      0          0  \n",
      "...         ...    ...        ...  \n",
      "3842          0      0          0  \n",
      "3843          0      0          0  \n",
      "3844          1      0          0  \n",
      "3845          0      0          1  \n",
      "3846          0      0          0  \n",
      "\n",
      "[3847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcfb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c98ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n",
      "1       送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！\n",
      "2                 茶的味道很纯正，使用方便，包装很好\n",
      "3                   茶叶不错，味道挺好的，5分好评\n",
      "4                          口感特别好~~！\n",
      "                   ...             \n",
      "3842                  很好，很新鲜，不错，好评！\n",
      "3843                 不错，是正品，下次还会再来。\n",
      "3844        老板态度好，发货及时，茶叶很好，口感很好，甘甜\n",
      "3845                       哎，，，，，，，\n",
      "3846                         味道真的不错\n",
      "Name: rateContent, Length: 3847, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[:,\"rateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2fa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n"
     ]
    }
   ],
   "source": [
    "training_data_list = []\n",
    "print(train.loc[:,\"rateContent\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc74bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[:,\"rateContent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f016d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    training_data_list.append(train.loc[:,\"rateContent\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_data_list.append(train.loc[:,\"rateContent\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    label_list.append(train.loc[:,\"package\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7598d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_label_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_label_list.append(train.loc[:,\"package\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ddd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36865e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba976637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(training_data_list)\n",
    "val_inputs, val_masks = preprocessing_for_bert(vali_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b9aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(label_list)\n",
    "val_labels = torch.tensor(vali_label_list)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e576971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class RNN1(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert=False ):\n",
    "      \n",
    "        super(RNN1, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 8\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            #BiLSTM\n",
    "            # nn.Linear(2*H, H),\n",
    "            #LSTM\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "    \n",
    "        self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=True)\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "  \n",
    "        \n",
    "\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        a = outputs[0].tolist()\n",
    "        #print(\"size out of bert:\", np.array(a).shape)\n",
    "\n",
    "        output =  self.bilstm(outputs[0])\n",
    "        #print(\"output of BiLSTM \",len(list(outputs[0])))\n",
    "         # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = output[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9704e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    RNN1 = RNN1(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    RNN1.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(RNN1.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return RNN1, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0618c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5a3a6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'RNN1' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)    \u001b[38;5;66;03m# Set seed for reproducibility\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m RNN1, optimizer, scheduler \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train(RNN1, train_dataloader, val_dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, evaluation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Instantiate Bert Classifier\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m RNN1 \u001b[38;5;241m=\u001b[39m \u001b[43mRNN1\u001b[49m(freeze_bert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Tell PyTorch to run the model on GPU\u001b[39;00m\n\u001b[1;32m     10\u001b[0m RNN1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'RNN1' referenced before assignment"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "RNN1, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(RNN1, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865099b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
