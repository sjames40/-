{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "#import fasttext\n",
    "#import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edeaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb71cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052074671668017\n",
      "0.7045793841375075\n",
      "0.8142993898625756\n",
      "0.7122102717678036\n"
     ]
    }
   ],
   "source": [
    "text = SnowNLP(u'口感很好，喝起来味道不错，包装也很精美，送人也很大气。')\n",
    "sent = text.sentences\n",
    "for sen in sent:\n",
    "    s = SnowNLP(sen)\n",
    "    print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['送', '人', '也', '很', '大', '气']\n"
     ]
    }
   ],
   "source": [
    "print(s.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3192e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('送', 'v'), ('人', 'n'), ('也', 'd'), ('很', 'd'), ('大', 'a'), ('气', 'n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e398a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"teapro.csv\", encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc8ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        no                  rateContent  package  quality  price  service  \\\n",
      "0        1  口感很好，喝起来味道不错，包装也很精美，送人也很大气。        1        1      0        0   \n",
      "1        2  送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！        1        0      0        0   \n",
      "2        3            茶的味道很纯正，使用方便，包装很好        1        1      0        0   \n",
      "3        4              茶叶不错，味道挺好的，5分好评        0        1      0        0   \n",
      "4        5                     口感特别好~~！        0        1      0        0   \n",
      "...    ...                          ...      ...      ...    ...      ...   \n",
      "3842  3843                很好，很新鲜，不错，好评！        0        0      0        0   \n",
      "3843  3844               不错，是正品，下次还会再来。        0        1      0        0   \n",
      "3844  3845      老板态度好，发货及时，茶叶很好，口感很好，甘甜        0        1      0        1   \n",
      "3845  3846                     哎，，，，，，，        0        0      0        0   \n",
      "3846  3847                       味道真的不错        0        1      0        0   \n",
      "\n",
      "      logistics  other  sentiment  \n",
      "0             0      0          0  \n",
      "1             0      0          1  \n",
      "2             0      0          0  \n",
      "3             0      0          0  \n",
      "4             0      0          0  \n",
      "...         ...    ...        ...  \n",
      "3842          0      0          0  \n",
      "3843          0      0          0  \n",
      "3844          1      0          0  \n",
      "3845          0      0          1  \n",
      "3846          0      0          0  \n",
      "\n",
      "[3847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcfb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c98ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n",
      "1       送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！\n",
      "2                 茶的味道很纯正，使用方便，包装很好\n",
      "3                   茶叶不错，味道挺好的，5分好评\n",
      "4                          口感特别好~~！\n",
      "                   ...             \n",
      "3842                  很好，很新鲜，不错，好评！\n",
      "3843                 不错，是正品，下次还会再来。\n",
      "3844        老板态度好，发货及时，茶叶很好，口感很好，甘甜\n",
      "3845                       哎，，，，，，，\n",
      "3846                         味道真的不错\n",
      "Name: rateContent, Length: 3847, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[:,\"rateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2fa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n"
     ]
    }
   ],
   "source": [
    "training_data_list = []\n",
    "print(train.loc[:,\"rateContent\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc74bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[:,\"rateContent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f016d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    training_data_list.append(train.loc[:,\"rateContent\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_data_list.append(train.loc[:,\"rateContent\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    label_list.append(train.loc[:,\"package\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7598d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_label_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_label_list.append(train.loc[:,\"package\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ddd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36865e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba976637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(training_data_list)\n",
    "val_inputs, val_masks = preprocessing_for_bert(vali_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b9aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(label_list)\n",
    "val_labels = torch.tensor(vali_label_list)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 1\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7b2f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Convlution，MaxPooling層からの出力次元の算出用関数\n",
    "def out_size(sequence_length, filter_size, padding = 0, dilation = 1, stride = 1):\n",
    "    length = sequence_length + 2 * padding - dilation * (filter_size - 1) - 1\n",
    "    length = int(length/stride)\n",
    "    return length + 1\n",
    "\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, params, gat = None):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        \n",
    "        poolingLayer_out_size = 0\n",
    "        \n",
    "        self.dropout = params['cnn_dropout']\n",
    "        self.filter_size = params['cnn_filter_sizes']\n",
    "        \n",
    "        if bool(self.dropout[0]) :\n",
    "            self.drp1 = nn.Dropout(p = self.dropout[0])\n",
    "        if bool(self.dropout[1]) :\n",
    "            self.drp2 = nn.Dropout(p = self.dropout[1])\n",
    "\n",
    "        for fsz in self.filter_size :        \n",
    "            l_conv = nn.Conv1d(params['embedding_dim'], params['cnn_out_channels'], fsz, stride = params['cnn_conv_stride'])\n",
    "            torch.nn.init.xavier_uniform_(l_conv.weight)\n",
    "\n",
    "            l_pool = nn.MaxPool1d(params['cnn_pool_stride'], stride = params['cnn_pool_stride'])\n",
    "            l_out_size = out_size(params['sequence_length'], fsz, stride = params['cnn_conv_stride'])\n",
    "            pool_out_size = int(l_out_size * params['cnn_out_channels'] / params['cnn_pool_stride']) \n",
    "            poolingLayer_out_size += pool_out_size\n",
    "\n",
    "            self.conv_layers.append(l_conv)\n",
    "            self.pool_layers.append(l_pool)\n",
    "\n",
    "        self.linear1 = nn.Linear(poolingLayer_out_size, params['cnn_hidden_dim1'])\n",
    "        self.linear2 = nn.Linear(params['cnn_hidden_dim1'], params['classes'])\n",
    "        #torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        #torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, texts):\n",
    "\n",
    "        texts = self.bert(texts)[0].detach_()\n",
    "\n",
    "        texts = texts.permute(0, 2, 1)\n",
    "\n",
    "        #if bool(self.dropout[0]):\n",
    "        #    texts = self.drp1(texts) \n",
    "        \n",
    "        conv_out = []\n",
    "\n",
    "        for i in range(len(self.filter_size)) :\n",
    "            outputs = self.conv_layers[i](texts)\n",
    "            outputs = outputs.view(outputs.shape[0], 1, outputs.shape[1] * outputs.shape[2])\n",
    "            outputs = self.pool_layers[i](outputs)\n",
    "            outputs = nn.functional.relu(outputs)\n",
    "            outputs = outputs.view(outputs.shape[0], -1)\n",
    "            conv_out.append(outputs)\n",
    "        #    del outputs\n",
    "\n",
    "        #if len(self.filter_size) > 1 :\n",
    "        #    outputs = torch.cat(conv_out, 1)\n",
    "        #else:\n",
    "        #    outputs = conv_out[0]\n",
    "\n",
    "        outputs = self.linear1(outputs)\n",
    "\n",
    "        #outputs = nn.functional.relu(outputs)\n",
    "\n",
    "        #if bool(self.dropout[1]) :\n",
    "        #    outputs = self.drp2(outputs)\n",
    "         \n",
    "        #outputs = self.linear2(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a07f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "873176a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb74ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        #model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            #print(b_labels.dtype)\n",
    "            #print(b_labels)\n",
    "            #print(b_labels.shape)\n",
    "            #b_labels = b_labels.float32\n",
    "            #b_labels = torch.tensor(b_labels,dtype=torch.float32)\n",
    "            \n",
    "            # Zero out any previously calculated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids)\n",
    "            #logits = torch.tensor(logits,dtype=torch.float32)\n",
    "            #print(logits.shape)\n",
    "            #print(logits.dtype)\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        #b_label = torch.float(b)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b2cc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38633f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'epoch' : 2,\n",
    "            'classes' : 103,\n",
    "            'embedding_dim' : 768,\n",
    "            'sequence_length' : 64,\n",
    "            'seed' : 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26b9e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params = {  \n",
    "            'cnn_out_channels' : 32,\n",
    "            'cnn_filter_sizes' : [3],\n",
    "            'cnn_hidden_dim1' : 368,\n",
    "            'cnn_conv_stride' : 2,\n",
    "            'cnn_pool_stride' : 8,\n",
    "            'cnn_dropout' : [False, False],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baea4480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   6.152628   |     -      |     -     |   0.25   \n",
      "   1    |   40    |   5.519648   |     -      |     -     |   0.18   \n",
      "   1    |   60    |   4.904882   |     -      |     -     |   0.18   \n",
      "   1    |   80    |   4.514578   |     -      |     -     |   0.18   \n",
      "   1    |   100   |   4.203449   |     -      |     -     |   0.18   \n",
      "   1    |   120   |   3.236603   |     -      |     -     |   0.18   \n",
      "   1    |   140   |   2.770659   |     -      |     -     |   0.18   \n",
      "   1    |   160   |   2.382719   |     -      |     -     |   0.18   \n",
      "   1    |   180   |   1.735832   |     -      |     -     |   0.18   \n",
      "   1    |   200   |   1.946089   |     -      |     -     |   0.18   \n",
      "   1    |   220   |   1.225204   |     -      |     -     |   0.18   \n",
      "   1    |   240   |   0.563662   |     -      |     -     |   0.18   \n",
      "   1    |   260   |   1.390809   |     -      |     -     |   0.18   \n",
      "   1    |   280   |   1.866828   |     -      |     -     |   0.18   \n",
      "   1    |   300   |   0.831061   |     -      |     -     |   0.18   \n",
      "   1    |   320   |   0.850145   |     -      |     -     |   0.18   \n",
      "   1    |   340   |   1.253931   |     -      |     -     |   0.18   \n",
      "   1    |   360   |   0.866591   |     -      |     -     |   0.18   \n",
      "   1    |   380   |   0.542613   |     -      |     -     |   0.18   \n",
      "   1    |   400   |   0.127165   |     -      |     -     |   0.18   \n",
      "   1    |   420   |   1.235538   |     -      |     -     |   0.18   \n",
      "   1    |   440   |   1.012689   |     -      |     -     |   0.18   \n",
      "   1    |   460   |   0.397364   |     -      |     -     |   0.18   \n",
      "   1    |   480   |   1.073419   |     -      |     -     |   0.18   \n",
      "   1    |   500   |   0.813150   |     -      |     -     |   0.18   \n",
      "   1    |   520   |   1.328721   |     -      |     -     |   0.18   \n",
      "   1    |   540   |   1.218537   |     -      |     -     |   0.18   \n",
      "   1    |   560   |   0.785948   |     -      |     -     |   0.18   \n",
      "   1    |   580   |   1.671287   |     -      |     -     |   0.18   \n",
      "   1    |   600   |   1.526540   |     -      |     -     |   0.18   \n",
      "   1    |   620   |   0.580610   |     -      |     -     |   0.18   \n",
      "   1    |   640   |   0.319240   |     -      |     -     |   0.18   \n",
      "   1    |   660   |   0.794294   |     -      |     -     |   0.18   \n",
      "   1    |   680   |   0.518088   |     -      |     -     |   0.18   \n",
      "   1    |   700   |   1.143730   |     -      |     -     |   0.18   \n",
      "   1    |   720   |   2.533306   |     -      |     -     |   0.18   \n",
      "   1    |   740   |   1.501140   |     -      |     -     |   0.18   \n",
      "   1    |   760   |   0.542682   |     -      |     -     |   0.18   \n",
      "   1    |   780   |   1.188037   |     -      |     -     |   0.18   \n",
      "   1    |   800   |   0.365344   |     -      |     -     |   0.18   \n",
      "   1    |   820   |   0.187379   |     -      |     -     |   0.18   \n",
      "   1    |   840   |   0.658633   |     -      |     -     |   0.18   \n",
      "   1    |   860   |   0.372758   |     -      |     -     |   0.18   \n",
      "   1    |   880   |   1.330657   |     -      |     -     |   0.18   \n",
      "   1    |   900   |   0.024754   |     -      |     -     |   0.18   \n",
      "   1    |   920   |   1.017719   |     -      |     -     |   0.18   \n",
      "   1    |   940   |   0.607447   |     -      |     -     |   0.18   \n",
      "   1    |   960   |   1.502991   |     -      |     -     |   0.18   \n",
      "   1    |   980   |   0.809901   |     -      |     -     |   0.18   \n",
      "   1    |  1000   |   0.339081   |     -      |     -     |   0.18   \n",
      "   1    |  1020   |   0.656510   |     -      |     -     |   0.18   \n",
      "   1    |  1040   |   0.687442   |     -      |     -     |   0.18   \n",
      "   1    |  1060   |   0.655699   |     -      |     -     |   0.18   \n",
      "   1    |  1080   |   0.598074   |     -      |     -     |   0.18   \n",
      "   1    |  1100   |   0.005159   |     -      |     -     |   0.18   \n",
      "   1    |  1120   |   0.649668   |     -      |     -     |   0.18   \n",
      "   1    |  1140   |   0.684009   |     -      |     -     |   0.18   \n",
      "   1    |  1160   |   0.345598   |     -      |     -     |   0.18   \n",
      "   1    |  1180   |   0.623638   |     -      |     -     |   0.18   \n",
      "   1    |  1200   |   1.589827   |     -      |     -     |   0.18   \n",
      "   1    |  1220   |   1.207192   |     -      |     -     |   0.18   \n",
      "   1    |  1240   |   0.996846   |     -      |     -     |   0.18   \n",
      "   1    |  1260   |   0.904140   |     -      |     -     |   0.18   \n",
      "   1    |  1280   |   0.479774   |     -      |     -     |   0.18   \n",
      "   1    |  1300   |   1.042891   |     -      |     -     |   0.18   \n",
      "   1    |  1320   |   2.777646   |     -      |     -     |   0.18   \n",
      "   1    |  1340   |   0.010848   |     -      |     -     |   0.18   \n",
      "   1    |  1360   |   0.003657   |     -      |     -     |   0.18   \n",
      "   1    |  1380   |   1.615797   |     -      |     -     |   0.18   \n",
      "   1    |  1400   |   0.335105   |     -      |     -     |   0.18   \n",
      "   1    |  1420   |   0.936111   |     -      |     -     |   0.18   \n",
      "   1    |  1440   |   0.302513   |     -      |     -     |   0.18   \n",
      "   1    |  1460   |   0.848581   |     -      |     -     |   0.18   \n",
      "   1    |  1480   |   1.221351   |     -      |     -     |   0.18   \n",
      "   1    |  1500   |   0.280455   |     -      |     -     |   0.18   \n",
      "   1    |  1520   |   0.002239   |     -      |     -     |   0.18   \n",
      "   1    |  1540   |   0.578579   |     -      |     -     |   0.18   \n",
      "   1    |  1560   |   0.948070   |     -      |     -     |   0.18   \n",
      "   1    |  1580   |   0.695610   |     -      |     -     |   0.18   \n",
      "   1    |  1600   |   1.332909   |     -      |     -     |   0.18   \n",
      "   1    |  1620   |   1.105588   |     -      |     -     |   0.18   \n",
      "   1    |  1640   |   0.661853   |     -      |     -     |   0.18   \n",
      "   1    |  1660   |   0.321414   |     -      |     -     |   0.18   \n",
      "   1    |  1680   |   0.842021   |     -      |     -     |   0.18   \n",
      "   1    |  1700   |   1.324415   |     -      |     -     |   0.18   \n",
      "   1    |  1720   |   1.004539   |     -      |     -     |   0.18   \n",
      "   1    |  1740   |   0.657767   |     -      |     -     |   0.18   \n",
      "   1    |  1760   |   1.468008   |     -      |     -     |   0.18   \n",
      "   1    |  1780   |   0.347272   |     -      |     -     |   0.18   \n",
      "   1    |  1800   |   0.580940   |     -      |     -     |   0.18   \n",
      "   1    |  1820   |   1.391988   |     -      |     -     |   0.18   \n",
      "   1    |  1840   |   0.317056   |     -      |     -     |   0.18   \n",
      "   1    |  1860   |   1.067538   |     -      |     -     |   0.18   \n",
      "   1    |  1880   |   0.349166   |     -      |     -     |   0.18   \n",
      "   1    |  1900   |   0.355460   |     -      |     -     |   0.18   \n",
      "   1    |  1920   |   1.377685   |     -      |     -     |   0.18   \n",
      "   1    |  1940   |   0.956443   |     -      |     -     |   0.18   \n",
      "   1    |  1960   |   1.265399   |     -      |     -     |   0.18   \n",
      "   1    |  1980   |   0.833527   |     -      |     -     |   0.18   \n",
      "   1    |  2000   |   0.511619   |     -      |     -     |   0.18   \n",
      "   1    |  2020   |   2.493420   |     -      |     -     |   0.18   \n",
      "   1    |  2040   |   1.028258   |     -      |     -     |   0.18   \n",
      "   1    |  2060   |   0.481340   |     -      |     -     |   0.18   \n",
      "   1    |  2080   |   0.605142   |     -      |     -     |   0.18   \n",
      "   1    |  2100   |   0.570400   |     -      |     -     |   0.18   \n",
      "   1    |  2120   |   0.935557   |     -      |     -     |   0.18   \n",
      "   1    |  2140   |   1.210500   |     -      |     -     |   0.18   \n",
      "   1    |  2160   |   0.336846   |     -      |     -     |   0.18   \n",
      "   1    |  2180   |   0.624918   |     -      |     -     |   0.18   \n",
      "   1    |  2200   |   0.607100   |     -      |     -     |   0.18   \n",
      "   1    |  2220   |   0.885329   |     -      |     -     |   0.18   \n",
      "   1    |  2240   |   0.640290   |     -      |     -     |   0.18   \n",
      "   1    |  2260   |   0.663237   |     -      |     -     |   0.18   \n",
      "   1    |  2280   |   1.160478   |     -      |     -     |   0.18   \n",
      "   1    |  2300   |   0.001955   |     -      |     -     |   0.18   \n",
      "   1    |  2320   |   0.302970   |     -      |     -     |   0.18   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  2340   |   0.472586   |     -      |     -     |   0.18   \n",
      "   1    |  2360   |   0.601694   |     -      |     -     |   0.18   \n",
      "   1    |  2380   |   1.767153   |     -      |     -     |   0.18   \n",
      "   1    |  2400   |   0.604893   |     -      |     -     |   0.18   \n",
      "   1    |  2420   |   0.557250   |     -      |     -     |   0.18   \n",
      "   1    |  2440   |   0.282813   |     -      |     -     |   0.18   \n",
      "   1    |  2460   |   0.378637   |     -      |     -     |   0.18   \n",
      "   1    |  2480   |   0.636703   |     -      |     -     |   0.18   \n",
      "   1    |  2500   |   0.285166   |     -      |     -     |   0.18   \n",
      "   1    |  2520   |   0.357550   |     -      |     -     |   0.18   \n",
      "   1    |  2540   |   0.984028   |     -      |     -     |   0.18   \n",
      "   1    |  2560   |   0.310705   |     -      |     -     |   0.18   \n",
      "   1    |  2580   |   0.641833   |     -      |     -     |   0.18   \n",
      "   1    |  2600   |   1.344804   |     -      |     -     |   0.18   \n",
      "   1    |  2620   |   0.257136   |     -      |     -     |   0.18   \n",
      "   1    |  2640   |   0.668493   |     -      |     -     |   0.18   \n",
      "   1    |  2660   |   0.641984   |     -      |     -     |   0.18   \n",
      "   1    |  2680   |   0.695881   |     -      |     -     |   0.18   \n",
      "   1    |  2700   |   0.884571   |     -      |     -     |   0.18   \n",
      "   1    |  2720   |   0.635239   |     -      |     -     |   0.18   \n",
      "   1    |  2740   |   1.004349   |     -      |     -     |   0.18   \n",
      "   1    |  2760   |   0.305206   |     -      |     -     |   0.18   \n",
      "   1    |  2780   |   0.861043   |     -      |     -     |   0.18   \n",
      "   1    |  2800   |   0.303401   |     -      |     -     |   0.18   \n",
      "   1    |  2820   |   1.219561   |     -      |     -     |   0.18   \n",
      "   1    |  2840   |   0.755081   |     -      |     -     |   0.18   \n",
      "   1    |  2860   |   1.198344   |     -      |     -     |   0.18   \n",
      "   1    |  2880   |   0.002230   |     -      |     -     |   0.18   \n",
      "   1    |  2900   |   1.216114   |     -      |     -     |   0.18   \n",
      "   1    |  2920   |   0.653972   |     -      |     -     |   0.18   \n",
      "   1    |  2940   |   0.949604   |     -      |     -     |   0.18   \n",
      "   1    |  2960   |   0.260770   |     -      |     -     |   0.18   \n",
      "   1    |  2980   |   0.843598   |     -      |     -     |   0.18   \n",
      "   1    |  3000   |   0.920978   |     -      |     -     |   0.18   \n",
      "   1    |  3020   |   1.218866   |     -      |     -     |   0.18   \n",
      "   1    |  3040   |   0.599558   |     -      |     -     |   0.18   \n",
      "   1    |  3060   |   1.792830   |     -      |     -     |   0.18   \n",
      "   1    |  3080   |   0.002862   |     -      |     -     |   0.18   \n",
      "   1    |  3100   |   1.017148   |     -      |     -     |   0.18   \n",
      "   1    |  3120   |   1.418563   |     -      |     -     |   0.18   \n",
      "   1    |  3140   |   1.243948   |     -      |     -     |   0.18   \n",
      "   1    |  3160   |   0.644209   |     -      |     -     |   0.18   \n",
      "   1    |  3180   |   1.146995   |     -      |     -     |   0.18   \n",
      "   1    |  3200   |   0.243955   |     -      |     -     |   0.18   \n",
      "   1    |  3220   |   0.294125   |     -      |     -     |   0.18   \n",
      "   1    |  3240   |   0.904311   |     -      |     -     |   0.18   \n",
      "   1    |  3260   |   0.631625   |     -      |     -     |   0.18   \n",
      "   1    |  3280   |   0.001902   |     -      |     -     |   0.18   \n",
      "   1    |  3300   |   0.616974   |     -      |     -     |   0.18   \n",
      "   1    |  3320   |   1.691541   |     -      |     -     |   0.18   \n",
      "   1    |  3340   |   0.878927   |     -      |     -     |   0.18   \n",
      "   1    |  3360   |   0.605280   |     -      |     -     |   0.18   \n",
      "   1    |  3380   |   0.617683   |     -      |     -     |   0.18   \n",
      "   1    |  3400   |   0.293902   |     -      |     -     |   0.18   \n",
      "   1    |  3420   |   0.756443   |     -      |     -     |   0.18   \n",
      "   1    |  3440   |   0.630041   |     -      |     -     |   0.18   \n",
      "   1    |  3460   |   0.332578   |     -      |     -     |   0.18   \n",
      "   1    |  3480   |   1.366880   |     -      |     -     |   0.18   \n",
      "   1    |  3500   |   0.253421   |     -      |     -     |   0.18   \n",
      "   1    |  3520   |   1.452672   |     -      |     -     |   0.18   \n",
      "   1    |  3540   |   1.382566   |     -      |     -     |   0.18   \n",
      "   1    |  3560   |   0.795962   |     -      |     -     |   0.18   \n",
      "   1    |  3580   |   0.759317   |     -      |     -     |   0.18   \n",
      "   1    |  3600   |   0.989181   |     -      |     -     |   0.18   \n",
      "   1    |  3620   |   0.241613   |     -      |     -     |   0.18   \n",
      "   1    |  3640   |   0.854119   |     -      |     -     |   0.18   \n",
      "   1    |  3660   |   0.617652   |     -      |     -     |   0.18   \n",
      "   1    |  3680   |   1.186622   |     -      |     -     |   0.18   \n",
      "   1    |  3700   |   0.334005   |     -      |     -     |   0.18   \n",
      "   1    |  3720   |   1.191530   |     -      |     -     |   0.18   \n",
      "   1    |  3740   |   0.875997   |     -      |     -     |   0.18   \n",
      "   1    |  3746   |   0.002843   |     -      |     -     |   0.05   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.962301   |  0.517067  |   91.00   |   33.70  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.824311   |     -      |     -     |   0.19   \n",
      "   2    |   40    |   1.374424   |     -      |     -     |   0.18   \n",
      "   2    |   60    |   0.823296   |     -      |     -     |   0.18   \n",
      "   2    |   80    |   0.338741   |     -      |     -     |   0.18   \n",
      "   2    |   100   |   1.272472   |     -      |     -     |   0.18   \n",
      "   2    |   120   |   1.061659   |     -      |     -     |   0.18   \n",
      "   2    |   140   |   0.824776   |     -      |     -     |   0.18   \n",
      "   2    |   160   |   0.707022   |     -      |     -     |   0.18   \n",
      "   2    |   180   |   0.746625   |     -      |     -     |   0.18   \n",
      "   2    |   200   |   1.127673   |     -      |     -     |   0.18   \n",
      "   2    |   220   |   1.009613   |     -      |     -     |   0.18   \n",
      "   2    |   240   |   0.788988   |     -      |     -     |   0.18   \n",
      "   2    |   260   |   1.263883   |     -      |     -     |   0.18   \n",
      "   2    |   280   |   0.970555   |     -      |     -     |   0.18   \n",
      "   2    |   300   |   0.448328   |     -      |     -     |   0.18   \n",
      "   2    |   320   |   0.754905   |     -      |     -     |   0.18   \n",
      "   2    |   340   |   0.001726   |     -      |     -     |   0.18   \n",
      "   2    |   360   |   0.902448   |     -      |     -     |   0.18   \n",
      "   2    |   380   |   0.540895   |     -      |     -     |   0.18   \n",
      "   2    |   400   |   0.003020   |     -      |     -     |   0.18   \n",
      "   2    |   420   |   0.001923   |     -      |     -     |   0.18   \n",
      "   2    |   440   |   1.519448   |     -      |     -     |   0.18   \n",
      "   2    |   460   |   0.618890   |     -      |     -     |   0.18   \n",
      "   2    |   480   |   0.788630   |     -      |     -     |   0.18   \n",
      "   2    |   500   |   0.919497   |     -      |     -     |   0.18   \n",
      "   2    |   520   |   1.289839   |     -      |     -     |   0.18   \n",
      "   2    |   540   |   0.543247   |     -      |     -     |   0.18   \n",
      "   2    |   560   |   0.860067   |     -      |     -     |   0.18   \n",
      "   2    |   580   |   0.645889   |     -      |     -     |   0.18   \n",
      "   2    |   600   |   0.301625   |     -      |     -     |   0.18   \n",
      "   2    |   620   |   0.001216   |     -      |     -     |   0.18   \n",
      "   2    |   640   |   0.549414   |     -      |     -     |   0.18   \n",
      "   2    |   660   |   0.530332   |     -      |     -     |   0.18   \n",
      "   2    |   680   |   0.826395   |     -      |     -     |   0.18   \n",
      "   2    |   700   |   1.218890   |     -      |     -     |   0.18   \n",
      "   2    |   720   |   0.343763   |     -      |     -     |   0.18   \n",
      "   2    |   740   |   0.538894   |     -      |     -     |   0.18   \n",
      "   2    |   760   |   0.275482   |     -      |     -     |   0.18   \n",
      "   2    |   780   |   1.362447   |     -      |     -     |   0.18   \n",
      "   2    |   800   |   1.045203   |     -      |     -     |   0.18   \n",
      "   2    |   820   |   0.841747   |     -      |     -     |   0.18   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |   840   |   0.995327   |     -      |     -     |   0.18   \n",
      "   2    |   860   |   0.280628   |     -      |     -     |   0.18   \n",
      "   2    |   880   |   0.458618   |     -      |     -     |   0.18   \n",
      "   2    |   900   |   1.248930   |     -      |     -     |   0.18   \n",
      "   2    |   920   |   0.422849   |     -      |     -     |   0.18   \n",
      "   2    |   940   |   0.002344   |     -      |     -     |   0.18   \n",
      "   2    |   960   |   0.630738   |     -      |     -     |   0.18   \n",
      "   2    |   980   |   1.260689   |     -      |     -     |   0.18   \n",
      "   2    |  1000   |   0.750658   |     -      |     -     |   0.18   \n",
      "   2    |  1020   |   1.075145   |     -      |     -     |   0.18   \n",
      "   2    |  1040   |   0.003070   |     -      |     -     |   0.18   \n",
      "   2    |  1060   |   0.305903   |     -      |     -     |   0.18   \n",
      "   2    |  1080   |   0.518408   |     -      |     -     |   0.18   \n",
      "   2    |  1100   |   0.835290   |     -      |     -     |   0.18   \n",
      "   2    |  1120   |   0.556883   |     -      |     -     |   0.18   \n",
      "   2    |  1140   |   0.668852   |     -      |     -     |   0.18   \n",
      "   2    |  1160   |   1.115232   |     -      |     -     |   0.18   \n",
      "   2    |  1180   |   0.716023   |     -      |     -     |   0.18   \n",
      "   2    |  1200   |   0.856903   |     -      |     -     |   0.18   \n",
      "   2    |  1220   |   0.477691   |     -      |     -     |   0.18   \n",
      "   2    |  1240   |   1.580970   |     -      |     -     |   0.18   \n",
      "   2    |  1260   |   1.185443   |     -      |     -     |   0.18   \n",
      "   2    |  1280   |   0.847262   |     -      |     -     |   0.18   \n",
      "   2    |  1300   |   1.256900   |     -      |     -     |   0.18   \n",
      "   2    |  1320   |   0.374453   |     -      |     -     |   0.18   \n",
      "   2    |  1340   |   0.590423   |     -      |     -     |   0.18   \n",
      "   2    |  1360   |   0.804002   |     -      |     -     |   0.18   \n",
      "   2    |  1380   |   0.001715   |     -      |     -     |   0.18   \n",
      "   2    |  1400   |   0.495880   |     -      |     -     |   0.18   \n",
      "   2    |  1420   |   0.309042   |     -      |     -     |   0.18   \n",
      "   2    |  1440   |   1.021764   |     -      |     -     |   0.18   \n",
      "   2    |  1460   |   0.855819   |     -      |     -     |   0.18   \n",
      "   2    |  1480   |   1.465938   |     -      |     -     |   0.18   \n",
      "   2    |  1500   |   0.846663   |     -      |     -     |   0.18   \n",
      "   2    |  1520   |   0.223162   |     -      |     -     |   0.18   \n",
      "   2    |  1540   |   0.745674   |     -      |     -     |   0.18   \n",
      "   2    |  1560   |   0.832085   |     -      |     -     |   0.18   \n",
      "   2    |  1580   |   1.059416   |     -      |     -     |   0.18   \n",
      "   2    |  1600   |   0.576169   |     -      |     -     |   0.18   \n",
      "   2    |  1620   |   1.183027   |     -      |     -     |   0.18   \n",
      "   2    |  1640   |   0.549048   |     -      |     -     |   0.18   \n",
      "   2    |  1660   |   1.115155   |     -      |     -     |   0.18   \n",
      "   2    |  1680   |   0.930302   |     -      |     -     |   0.18   \n",
      "   2    |  1700   |   0.700253   |     -      |     -     |   0.18   \n",
      "   2    |  1720   |   0.761901   |     -      |     -     |   0.18   \n",
      "   2    |  1740   |   0.448215   |     -      |     -     |   0.18   \n",
      "   2    |  1760   |   0.559799   |     -      |     -     |   0.18   \n",
      "   2    |  1780   |   0.283169   |     -      |     -     |   0.18   \n",
      "   2    |  1800   |   0.336750   |     -      |     -     |   0.18   \n",
      "   2    |  1820   |   0.336386   |     -      |     -     |   0.18   \n",
      "   2    |  1840   |   0.578957   |     -      |     -     |   0.18   \n",
      "   2    |  1860   |   0.640012   |     -      |     -     |   0.18   \n",
      "   2    |  1880   |   0.318586   |     -      |     -     |   0.18   \n",
      "   2    |  1900   |   0.808425   |     -      |     -     |   0.18   \n",
      "   2    |  1920   |   0.955284   |     -      |     -     |   0.18   \n",
      "   2    |  1940   |   0.001503   |     -      |     -     |   0.18   \n",
      "   2    |  1960   |   1.657773   |     -      |     -     |   0.18   \n",
      "   2    |  1980   |   0.228725   |     -      |     -     |   0.18   \n",
      "   2    |  2000   |   0.855626   |     -      |     -     |   0.18   \n",
      "   2    |  2020   |   0.307032   |     -      |     -     |   0.18   \n",
      "   2    |  2040   |   0.369885   |     -      |     -     |   0.18   \n",
      "   2    |  2060   |   0.832171   |     -      |     -     |   0.18   \n",
      "   2    |  2080   |   1.255610   |     -      |     -     |   0.18   \n",
      "   2    |  2100   |   1.296507   |     -      |     -     |   0.18   \n",
      "   2    |  2120   |   0.753628   |     -      |     -     |   0.18   \n",
      "   2    |  2140   |   0.772490   |     -      |     -     |   0.18   \n",
      "   2    |  2160   |   0.299343   |     -      |     -     |   0.18   \n",
      "   2    |  2180   |   0.579645   |     -      |     -     |   0.18   \n",
      "   2    |  2200   |   0.001639   |     -      |     -     |   0.18   \n",
      "   2    |  2220   |   0.911422   |     -      |     -     |   0.18   \n",
      "   2    |  2240   |   0.326401   |     -      |     -     |   0.18   \n",
      "   2    |  2260   |   0.262507   |     -      |     -     |   0.18   \n",
      "   2    |  2280   |   0.873672   |     -      |     -     |   0.18   \n",
      "   2    |  2300   |   0.611590   |     -      |     -     |   0.18   \n",
      "   2    |  2320   |   0.477922   |     -      |     -     |   0.18   \n",
      "   2    |  2340   |   0.317067   |     -      |     -     |   0.18   \n",
      "   2    |  2360   |   0.964458   |     -      |     -     |   0.18   \n",
      "   2    |  2380   |   0.258848   |     -      |     -     |   0.18   \n",
      "   2    |  2400   |   0.683500   |     -      |     -     |   0.18   \n",
      "   2    |  2420   |   0.891563   |     -      |     -     |   0.18   \n",
      "   2    |  2440   |   0.604508   |     -      |     -     |   0.18   \n",
      "   2    |  2460   |   0.795565   |     -      |     -     |   0.18   \n",
      "   2    |  2480   |   0.753240   |     -      |     -     |   0.18   \n",
      "   2    |  2500   |   0.276545   |     -      |     -     |   0.18   \n",
      "   2    |  2520   |   1.347088   |     -      |     -     |   0.18   \n",
      "   2    |  2540   |   0.449565   |     -      |     -     |   0.18   \n",
      "   2    |  2560   |   0.541431   |     -      |     -     |   0.18   \n",
      "   2    |  2580   |   0.487477   |     -      |     -     |   0.18   \n",
      "   2    |  2600   |   0.475966   |     -      |     -     |   0.18   \n",
      "   2    |  2620   |   0.732368   |     -      |     -     |   0.18   \n",
      "   2    |  2640   |   0.697841   |     -      |     -     |   0.18   \n",
      "   2    |  2660   |   0.572298   |     -      |     -     |   0.18   \n",
      "   2    |  2680   |   0.510070   |     -      |     -     |   0.18   \n",
      "   2    |  2700   |   0.256042   |     -      |     -     |   0.18   \n",
      "   2    |  2720   |   0.626239   |     -      |     -     |   0.18   \n",
      "   2    |  2740   |   0.551176   |     -      |     -     |   0.18   \n",
      "   2    |  2760   |   0.322815   |     -      |     -     |   0.18   \n",
      "   2    |  2780   |   0.347792   |     -      |     -     |   0.18   \n",
      "   2    |  2800   |   0.948226   |     -      |     -     |   0.18   \n",
      "   2    |  2820   |   0.991459   |     -      |     -     |   0.18   \n",
      "   2    |  2840   |   0.558456   |     -      |     -     |   0.18   \n",
      "   2    |  2860   |   0.503716   |     -      |     -     |   0.18   \n",
      "   2    |  2880   |   0.299228   |     -      |     -     |   0.18   \n",
      "   2    |  2900   |   0.453863   |     -      |     -     |   0.18   \n",
      "   2    |  2920   |   0.831581   |     -      |     -     |   0.18   \n",
      "   2    |  2940   |   0.697453   |     -      |     -     |   0.18   \n",
      "   2    |  2960   |   0.547424   |     -      |     -     |   0.18   \n",
      "   2    |  2980   |   1.060944   |     -      |     -     |   0.18   \n",
      "   2    |  3000   |   1.066268   |     -      |     -     |   0.18   \n",
      "   2    |  3020   |   0.333144   |     -      |     -     |   0.18   \n",
      "   2    |  3040   |   1.109485   |     -      |     -     |   0.18   \n",
      "   2    |  3060   |   1.717336   |     -      |     -     |   0.18   \n",
      "   2    |  3080   |   1.140343   |     -      |     -     |   0.18   \n",
      "   2    |  3100   |   0.694666   |     -      |     -     |   0.18   \n",
      "   2    |  3120   |   0.757173   |     -      |     -     |   0.18   \n",
      "   2    |  3140   |   0.548324   |     -      |     -     |   0.18   \n",
      "   2    |  3160   |   0.412637   |     -      |     -     |   0.18   \n",
      "   2    |  3180   |   0.177344   |     -      |     -     |   0.18   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  3200   |   1.190914   |     -      |     -     |   0.18   \n",
      "   2    |  3220   |   1.127388   |     -      |     -     |   0.18   \n",
      "   2    |  3240   |   0.434916   |     -      |     -     |   0.18   \n",
      "   2    |  3260   |   0.630316   |     -      |     -     |   0.18   \n",
      "   2    |  3280   |   0.444083   |     -      |     -     |   0.18   \n",
      "   2    |  3300   |   0.432425   |     -      |     -     |   0.18   \n",
      "   2    |  3320   |   0.583931   |     -      |     -     |   0.18   \n",
      "   2    |  3340   |   0.556159   |     -      |     -     |   0.18   \n",
      "   2    |  3360   |   0.833811   |     -      |     -     |   0.18   \n",
      "   2    |  3380   |   0.281632   |     -      |     -     |   0.18   \n",
      "   2    |  3400   |   1.044794   |     -      |     -     |   0.18   \n",
      "   2    |  3420   |   1.350985   |     -      |     -     |   0.18   \n",
      "   2    |  3440   |   0.453944   |     -      |     -     |   0.18   \n",
      "   2    |  3460   |   0.297128   |     -      |     -     |   0.18   \n",
      "   2    |  3480   |   0.530670   |     -      |     -     |   0.18   \n",
      "   2    |  3500   |   0.237012   |     -      |     -     |   0.18   \n",
      "   2    |  3520   |   0.315946   |     -      |     -     |   0.18   \n",
      "   2    |  3540   |   0.411190   |     -      |     -     |   0.18   \n",
      "   2    |  3560   |   1.299898   |     -      |     -     |   0.18   \n",
      "   2    |  3580   |   0.572945   |     -      |     -     |   0.18   \n",
      "   2    |  3600   |   1.120053   |     -      |     -     |   0.18   \n",
      "   2    |  3620   |   1.386948   |     -      |     -     |   0.18   \n",
      "   2    |  3640   |   0.462947   |     -      |     -     |   0.18   \n",
      "   2    |  3660   |   0.702447   |     -      |     -     |   0.18   \n",
      "   2    |  3680   |   0.954224   |     -      |     -     |   0.18   \n",
      "   2    |  3700   |   0.494692   |     -      |     -     |   0.18   \n",
      "   2    |  3720   |   0.737801   |     -      |     -     |   0.18   \n",
      "   2    |  3740   |   0.872579   |     -      |     -     |   0.18   \n",
      "   2    |  3746   |   0.823574   |     -      |     -     |   0.05   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.697113   |  0.477898  |   91.00   |   33.64  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "#bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "#params.update(learning_params)\n",
    "params.update(cnn_params)\n",
    "net = CNN(params)\n",
    "\n",
    "net = net.to(device)\n",
    "optimizer = AdamW(net.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "total_steps = len(train_dataloader) *4\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "train(net, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "642aa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "             logits = model(b_input_ids)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef509b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6546ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70d306ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8254\n",
      "Accuracy: 91.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(net, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, val_labels)\n",
    "prin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44853e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
