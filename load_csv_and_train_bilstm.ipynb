{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "#import fasttext\n",
    "#import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edeaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb71cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052074671668017\n",
      "0.7045793841375075\n",
      "0.8142993898625756\n",
      "0.7122102717678036\n"
     ]
    }
   ],
   "source": [
    "text = SnowNLP(u'口感很好，喝起来味道不错，包装也很精美，送人也很大气。')\n",
    "sent = text.sentences\n",
    "for sen in sent:\n",
    "    s = SnowNLP(sen)\n",
    "    print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['送', '人', '也', '很', '大', '气']\n"
     ]
    }
   ],
   "source": [
    "print(s.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3192e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('送', 'v'), ('人', 'n'), ('也', 'd'), ('很', 'd'), ('大', 'a'), ('气', 'n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e398a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"teapro.csv\", encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc8ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        no                  rateContent  package  quality  price  service  \\\n",
      "0        1  口感很好，喝起来味道不错，包装也很精美，送人也很大气。        1        1      0        0   \n",
      "1        2  送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！        1        0      0        0   \n",
      "2        3            茶的味道很纯正，使用方便，包装很好        1        1      0        0   \n",
      "3        4              茶叶不错，味道挺好的，5分好评        0        1      0        0   \n",
      "4        5                     口感特别好~~！        0        1      0        0   \n",
      "...    ...                          ...      ...      ...    ...      ...   \n",
      "3842  3843                很好，很新鲜，不错，好评！        0        0      0        0   \n",
      "3843  3844               不错，是正品，下次还会再来。        0        1      0        0   \n",
      "3844  3845      老板态度好，发货及时，茶叶很好，口感很好，甘甜        0        1      0        1   \n",
      "3845  3846                     哎，，，，，，，        0        0      0        0   \n",
      "3846  3847                       味道真的不错        0        1      0        0   \n",
      "\n",
      "      logistics  other  sentiment  \n",
      "0             0      0          0  \n",
      "1             0      0          1  \n",
      "2             0      0          0  \n",
      "3             0      0          0  \n",
      "4             0      0          0  \n",
      "...         ...    ...        ...  \n",
      "3842          0      0          0  \n",
      "3843          0      0          0  \n",
      "3844          1      0          0  \n",
      "3845          0      0          1  \n",
      "3846          0      0          0  \n",
      "\n",
      "[3847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcfb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c98ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n",
      "1       送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！\n",
      "2                 茶的味道很纯正，使用方便，包装很好\n",
      "3                   茶叶不错，味道挺好的，5分好评\n",
      "4                          口感特别好~~！\n",
      "                   ...             \n",
      "3842                  很好，很新鲜，不错，好评！\n",
      "3843                 不错，是正品，下次还会再来。\n",
      "3844        老板态度好，发货及时，茶叶很好，口感很好，甘甜\n",
      "3845                       哎，，，，，，，\n",
      "3846                         味道真的不错\n",
      "Name: rateContent, Length: 3847, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[:,\"rateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2fa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n"
     ]
    }
   ],
   "source": [
    "training_data_list = []\n",
    "print(train.loc[:,\"rateContent\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc74bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[:,\"rateContent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f016d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    training_data_list.append(train.loc[:,\"rateContent\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_data_list.append(train.loc[:,\"rateContent\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    label_list.append(train.loc[:,\"package\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7598d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_label_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_label_list.append(train.loc[:,\"package\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ddd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36865e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba976637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(training_data_list)\n",
    "val_inputs, val_masks = preprocessing_for_bert(vali_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b9aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(label_list)\n",
    "val_labels = torch.tensor(vali_label_list)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e576971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "# Bert-Bilstm-Classfier class\n",
    "class BertBilstmClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert=False ):\n",
    "      \n",
    "        super(BertBilstmClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 8\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            #BiLSTM\n",
    "            # nn.Linear(2*H, H),\n",
    "            #LSTM\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        # LSTM\n",
    "        self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=False)\n",
    "        # BiLSTM\n",
    "        # self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=True)\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "  \n",
    "        \n",
    "\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        a = outputs[0].tolist()\n",
    "        #print(\"size out of bert:\", np.array(a).shape)\n",
    "\n",
    "        output =  self.bilstm(outputs[0])\n",
    "        #print(\"output of BiLSTM \",len(list(outputs[0])))\n",
    "         # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = output[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9704e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertBilstmClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0618c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5a3a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.782670   |     -      |     -     |   1.81   \n",
      "   1    |   40    |   1.409482   |     -      |     -     |   1.62   \n",
      "   1    |   60    |   1.287555   |     -      |     -     |   1.63   \n",
      "   1    |   80    |   1.239199   |     -      |     -     |   1.61   \n",
      "   1    |   100   |   1.162499   |     -      |     -     |   1.61   \n",
      "   1    |   120   |   1.072002   |     -      |     -     |   1.61   \n",
      "   1    |   140   |   1.055673   |     -      |     -     |   1.60   \n",
      "   1    |   160   |   0.970645   |     -      |     -     |   1.60   \n",
      "   1    |   180   |   0.919316   |     -      |     -     |   1.60   \n",
      "   1    |   200   |   0.899355   |     -      |     -     |   1.62   \n",
      "   1    |   220   |   0.838591   |     -      |     -     |   1.61   \n",
      "   1    |   234   |   0.851097   |     -      |     -     |   1.10   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   1.133778   |  0.780255  |   91.96   |   19.22  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.820462   |     -      |     -     |   1.68   \n",
      "   2    |   40    |   0.785119   |     -      |     -     |   1.60   \n",
      "   2    |   60    |   0.767774   |     -      |     -     |   1.60   \n",
      "   2    |   80    |   0.740350   |     -      |     -     |   1.60   \n",
      "   2    |   100   |   0.722363   |     -      |     -     |   1.60   \n",
      "   2    |   120   |   0.724036   |     -      |     -     |   1.60   \n",
      "   2    |   140   |   0.685946   |     -      |     -     |   1.59   \n",
      "   2    |   160   |   0.665018   |     -      |     -     |   1.53   \n",
      "   2    |   180   |   0.677532   |     -      |     -     |   1.51   \n",
      "   2    |   200   |   0.711461   |     -      |     -     |   1.51   \n",
      "   2    |   220   |   0.714660   |     -      |     -     |   1.50   \n",
      "   2    |   234   |   0.690956   |     -      |     -     |   1.03   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.726758   |  0.632126  |   91.96   |   18.54  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "BertBilstmClassifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(BertBilstmClassifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "865099b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be671ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f4bf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c2afa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9560\n",
      "Accuracy: 91.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyT0lEQVR4nO3dd5xU5fXH8c8BaSKigjGGokRRKSItIDYQRRFBVBTRWLChsdefJsZYYmKMxliCBdRgA6IYEStGAREVBaSDIIJSFAuCghRZOL8/nrvusO7ODuzO3JnZ7/v1mtfOLXPvmbu7c+Z5nnvPNXdHRESkNFXiDkBERLKbEoWIiCSlRCEiIkkpUYiISFJKFCIikpQShYiIJKVEIVvFzGabWZe448gWZvYHM3skpn0PMbPb4th3RTOz35rZ69v4Wv1NppkSRQ4zs0/NbJ2ZrTGz5dEHxw7p3Ke7t3D3cencRyEzq2Fmt5vZ4uh9fmxm15qZZWL/JcTTxcyWJs5z97+6+3lp2p+Z2WVmNsvMfjCzpWb2rJntn479bSszu9nMnirPNtz9aXc/KoV9/Sw5ZvJvsrJSosh9vdx9B6A10Ab4fbzhbD0z266URc8CRwA9gDrAGcAA4N40xGBmlm3/D/cClwOXAbsA+wAjgWMrekdJfgdpF+e+JUXurkeOPoBPgSMTpv8OvJwwfSDwLrAKmA50SVi2C/Bv4HNgJTAyYVlPYFr0uneBVsX3CfwKWAfskrCsDfANUC2aPgeYG21/NLBHwroOXAx8DCwq4b0dAawHGhWb3xHYBOwdTY8Dbgc+AL4HXigWU7JjMA74C/BO9F72Bs6OYl4NLAQuiNatHa2zGVgTPX4F3Aw8Fa2zZ/S+zgIWR8fihoT91QIej47HXOD/gKWl/G6bRu+zQ5Lf/xBgIPByFO/7wF4Jy+8FlkTHZQpwaMKym4ERwFPR8vOADsB70bH6AvgXUD3hNS2A/wHfAl8CfwC6Az8CG6NjMj1aty7waLSdZcBtQNVoWf/omP8TWBEt6w9MiJZbtOyrKLaZQEvCl4SN0f7WAC8W/z8AqkZxfRIdkykU+xvSYxs+a+IOQI9y/PK2/AdpGP1D3RtNN4j+CXsQWo7douldo+UvA/8BdgaqAZ2j+W2if9CO0T/dWdF+apSwzzHA+Qnx3Ak8FD3vDSwAmgHbAX8E3k1Y16MPnV2AWiW8t78Bb5Xyvj+j6AN8XPRB1JLwYf4cRR/cZR2DcYQP9BZRjNUI39b3ij6sOgNrgbbR+l0o9sFOyYliMCEpHABsAJolvqfomDcEZhTfXsJ2LwQ+K+P3PyR6Px2i+J8GhicsPx2oFy27GlgO1EyIeyNwfHRsagHtCIl1u+i9zAWuiNavQ/jQvxqoGU13LH4MEvb9PPBw9Dv5BSGRF/7O+gMFwKXRvmqxZaI4mvABv1P0e2gG7J7wnm9L8n9wLeH/YN/otQcA9eL+X831R+wB6FGOX174B1lD+ObkwJvATtGy64Ani60/mvDBvzvhm/HOJWzzQeDPxebNoyiRJP5TngeMiZ4b4dvrYdH0q8C5CduoQvjQ3SOadqBrkvf2SOKHXrFlE4m+qRM+7P+WsKw54Rtn1WTHIOG1t5ZxjEcCl0fPu5BaomiYsPwDoF/0fCFwdMKy84pvL2HZDcDEMmIbAjySMN0D+CjJ+iuBAxLiHl/G9q8Ano+enwpMLWW9n45BNL0bIUHWSph3KjA2et4fWFxsG/0pShRdgfmEpFWlhPecLFHMA3qX939Ljy0f2dYnK1vveHevQ/gQ2w+oH83fAzjZzFYVPoBDCEmiEfCtu68sYXt7AFcXe10jQjdLcc8Bncxsd+AwQvJ5O2E79yZs41tCMmmQ8PolSd7XN1GsJdk9Wl7Sdj4jtAzqk/wYlBiDmR1jZhPN7Nto/R4UHdNULU94vhYoPMHgV8X2l+z9r6D095/KvjCza8xsrpl9F72Xumz5Xoq/933M7KXoxIjvgb8mrN+I0J2Tij0Iv4MvEo77w4SWRYn7TuTuYwjdXgOBr8xskJntmOK+tyZOSZESRZ5w97cI37buimYtIXyb3inhUdvd/xYt28XMdiphU0uAvxR73fbuPqyEfa4EXgdOAU4jtAA8YTsXFNtOLXd/N3ETSd7SG0BHM2uUONPMOhI+DMYkzE5cpzGhS+WbMo7Bz2IwsxqE5HcXsJu77wS8QkhwZcWbii8IXU4lxV3cm0BDM2u/LTsys0MJYyB9CS3HnYDvKHov8PP38yDwEdDU3Xck9PUXrr8E+HUpuyu+nSWEFkX9hOO+o7u3SPKaLTfofp+7tyO0EPchdCmV+bpo33uVsY5sJSWK/HIP0M3MDiAMUvYys6PNrKqZ1YxO72zo7l8QuoYeMLOdzayamR0WbWMwcKGZdYzOBKptZseaWZ1S9jkUOBM4KXpe6CHg92bWAsDM6prZyam+EXd/g/Bh+ZyZtYjew4HR+3rQ3T9OWP10M2tuZtsDtwIj3H1TsmNQym6rAzWAr4ECMzsGSDxl80ugnpnVTfV9FPMM4ZjsbGYNgEtKWzF6fw8Aw6KYq0fx9zOz61PYVx3COMDXwHZm9iegrG/ldQiDx2vMbD/gdwnLXgJ2N7MrotOW60RJG8Jx2bPwrLHo7+t14B9mtqOZVTGzvcyscwpxY2a/if7+qgE/EE5q2Jywr9ISFoQuyz+bWdPo77eVmdVLZb9SOiWKPOLuXwNPAH9y9yWEAeU/ED4slhC+lRX+zs8gfPP+iDB4fUW0jcnA+YSm/0rCgHT/JLsdRThDZ7m7T0+I5XngDmB41I0xCzhmK99SH2As8BphLOYpwpk0lxZb70lCa2o5YaD1siiGso7BFtx9dfTaZwjv/bTo/RUu/wgYBiyMulRK6o5L5lZgKbCI0GIaQfjmXZrLKOqCWUXoUjkBeDGFfY0mHLf5hO649STv6gK4hvCeVxO+MPyncEF0bLoBvQjH+WPg8Gjxs9HPFWb2YfT8TELinUM4liNIrSsNQkIbHL3uM0I33J3RskeB5tHxH1nCa+8m/P5eJyS9RwmD5VIOVtRTIJJ7zGwcYSA1lqujy8PMfkcY6E7pm7ZIXNSiEMkQM9vdzA6OumL2JZxq+nzccYmUJW2JwsweM7OvzGxWKcvNzO4zswVmNsPM2qYrFpEsUZ1w9s9qwmD8C4RxCJGslraup2hwdA3whLu3LGF5D0Jfcw/CxV33unvH4uuJiEi80taicPfxhHPnS9ObkETc3ScCO0Xn44uISBaJsxhXA7Y8C2NpNO+L4iua2QBCnRdq167dbr/99stIgLKlefNg3TqopXNIRHLGbhs+Y4eCVUz3gm/cfddt2UZOVG1090HAIID27dv75MmTY46ocurSJfwcNy7OKESkTIVDCmbw4IPw1VfYzTd/tq2bi/Osp2VseWVqw2ieiIhsq2XLoHdvGBpd//q738FNN5Vrk3EmilHAmdHZTwcC30VXdIqIyNZyh8GDoXlzeOMNWLOmwjadtq4nMxtGKFRX38JdwW4iFArD3R8i1NDpQbjydy3hPgAiIrK1PvkEzj8fxo6Fww8PCWOviit5lbZE4e6nlrHcCTeuERGR8pg5E6ZMgUGD4LzzwthEBcqJwWwRESlm1iz48EM480w4/nhYuBDqpaf+oUp4iIjkkh9/hJtvhrZt4YYbYP36MD9NSQKUKEREcsf774cEccstcMopMHUq1KyZ9t2q60lEJBcsWwaHHgq77QYvvQTHHpuxXatFISKSzebPDz8bNID//Admz85okgAlChGR7LRqFQwYAPvtB+PHh3knnAA7pnr78IqjricRkWwzalS4onr5crj2WvjNb2INR4lCRCSbnHcePPoo7L8/vPACtG8fd0RKFCIisUss4te+PeyxB1x3HVSvHm9cESUKEZE4LVkCF14I/frBGWeE51lGg9kiInHYvDmUAG/RItTu37Ah7ohKpRaFiEimffxxGIsYPx6OPDLUaGrSJO6oSqVEISKSaXPmwIwZ8Nhj0L9/hRfxq2hKFCIimTB9OkybBmedFW4stHAh7Lxz3FGlRGMUIiLptGED3HhjOJvpxhuLivjlSJIAJQoRkfR57z1o0wZuuw1OOy1jRfwqmrqeRETSYdky6NwZfvlLeOUVOOaYuCPaZmpRiIhUpLlzw88GDeCZZ0IRvxxOEpDHLYpBg2Do0LijyC/TpkHr1nFHIZKlVq6Eq6+Gf/87nPZ66KHhznN5IG9bFEOHhg82qTitW4duVhEp5vnnoXlzeOIJ+P3vYy/iV9HytkUB4YNt3Li4oxCRvHbOOaEV0bo1vPxyuANdnsnrRCEikhaJRfwOPBCaNoVrroFq1eKNK02UKEREtsZnn8EFF4R+2DPPDDcXynN5O0YhIlKhNm+GgQOhZUuYMAE2bow7ooxRi0JEpCzz5oUifhMmwFFHwcMPw557xh1VxihRiIiUZd68cD3EkCGhuynLi/hVNCUKEZGSTJ0azrE/+2w47rhQxG+nneKOKhYaoxARSbR+PfzhD+FaiJtvLiriV0mTBChRiIgUeeedcD3E7beHLqZp03KyiF9FU9eTiAiEIn6HHx5qNI0eHQatBVCLQkQquzlzws8GDeC552DmTCWJYpQoRKRy+vbbcBvSFi1CET+AXr1ghx1iDSsbqetJRCqf556Diy+GFSvghhugQ4e4I8pqShQiUrn07w+PPx6K9732mmrnp0CJQkTyX2IRv4MOgmbNwr0jttNHYCrSOkZhZt3NbJ6ZLTCz60tY3tjMxprZVDObYWY90hmPiFRCixaFweknngjTAwbAddcpSWyFtCUKM6sKDASOAZoDp5pZ82Kr/RF4xt3bAP2AB9IVj4hUMps2wX33hSJ+EycWtSpkq6WzRdEBWODuC939R2A40LvYOg7sGD2vC3yexnhEpLKYOzfcivTyy6Fz51CnqX//uKPKWelsezUAliRMLwU6FlvnZuB1M7sUqA0cWdKGzGwAMACgcePGFR6oiOSZBQtCIb8nn4Tf/rbSFfGraHFfR3EqMMTdGwI9gCfN7Gcxufsgd2/v7u133XXXjAcpIjlgyhR47LHwvFevMDZx+ulKEhUgnYliGdAoYbphNC/RucAzAO7+HlATqJ/GmEQk36xbB9dfDx07wp//XFTEb8cdk79OUpbORDEJaGpmTcysOmGwelSxdRYDRwCYWTNCovg6jTGJSD4ZPx4OOADuuCOMQUydqiJ+aZC2MQp3LzCzS4DRQFXgMXefbWa3ApPdfRRwNTDYzK4kDGz3d9epCSKSgmXL4IgjoFEjeOON8FzSIq0nErv7K8Arxeb9KeH5HODgdMYgInlm5kzYf/9QxO/550PF19q1444qr8U9mC0ikppvvoEzzoBWrYqK+PXsqSSRAbo0UUSymzs8+yxccgmsXAk33RQGriVjlChEJLuddVa4HqJ9e3jzzdDtJBmlRCEi2SexiF/nzqG76YorVJ8pJhqjEJHssnAhHHkkDBkSps89F665RkkiRkoUIpIdNm2Ce+4JXUuTJkEVfTxlC6VoEYnfnDlwzjnw/vtw7LHw0EPQsGHcUUlEiUJE4rdoEXzyCQwdCv36qT5TllGiEJF4TJoE06bB+eeHVsTChVCnTtxRSQnUCSgimbV2bRicPvBAuP32oiJ+ShJZS4lCRDJn3Lhwqus//hFaEirilxPU9SQimbF0KXTrBnvsAWPGhBpNkhPUohCR9Jo+Pfxs2BBeeAFmzFCSyDFKFCKSHl9/DaedBq1bw1tvhXk9esD228calmw9dT2JSMVyh+HD4bLL4Lvv4JZboFOnuKOSclCiEJGKdcYZ8PTTocLro49CixZxRyTllHKiMLPt3X1tOoMRkRy1eXO4SM4sjD+0axdaFFWrxh2ZVIAyxyjM7CAzmwN8FE0fYGYPpD0yEckNCxaE25D++99h+txz4corlSTySCqD2f8EjgZWALj7dOCwdAYlIjmgoADuuisU8Zs6FapXjzsiSZOUup7cfYltWXtlU3rCEZGcMGsWnH02TJ4MvXvDAw/Ar34Vd1SSJqkkiiVmdhDgZlYNuByYm96wRCSrLV4Mn30Wzm7q21dF/PJcKoniQuBeoAGwDHgduCidQYlIFnr//XDx3IAB4XqIhQthhx3ijkoyIJVEsa+7/zZxhpkdDLyTnpC2zaBBoUJxoWnTwnU+IlJOP/wAN94Ybir061+He1jXqKEkUYmkMph9f4rzYjV0aEgOhVq3DheFikg5jBkTivj9859w4YXw4YchSUilUmqLwsw6AQcBu5rZVQmLdgSy8ry31q1DcUoRqQBLl8LRR0OTJqEEx2E62bGyStb1VB3YIVonsVD898BJ6QxKRGI0dSq0aROK+L34InTuDLVqxR2VxKjUROHubwFvmdkQd/8sgzGJSBy+/DJcTf3MM6Fp3rkzdO8ed1SSBVIZzF5rZncCLYCf7jDi7l3TFpWIZI57qM10+eWwZg3cdhscdFDcUUkWSWUw+2lC+Y4mwC3Ap8CkNMYkIpl02mmhkN+++4YzQm64AapVizsqySKptCjqufujZnZ5QneUEoVILkss4nfUUaEM+MUXqz6TlCiVFsXG6OcXZnasmbUBdkljTCKSTvPnhwqvjz0Wps8+W5VeJalUWhS3mVld4GrC9RM7AlekMygRSYOCArj7brjpJqhZU2cyScrKTBTu/lL09DvgcPjpymwRyRUzZsA558CUKXDCCTBwIOy+e9xRSY5IdsFdVaAvocbTa+4+y8x6An8AagFtMhOiiJTb0qWwZAk8+yz06aMifrJVko1RPAqcB9QD7jOzp4C7gL+7e0pJwsy6m9k8M1tgZteXsk5fM5tjZrPNbGhJ64jINnj3XXjoofC8sIjfSScpSchWS9b11B5o5e6bzawmsBzYy91XpLLhqEUyEOgGLAUmmdkod5+TsE5T4PfAwe6+0sx+sa1vREQia9aEU1zvvx/22isMVteoAbVrxx2Z5KhkLYof3X0zgLuvBxammiQiHYAF7r7Q3X8EhgO9i61zPjDQ3VdG+/lqK7YvIsW9/jq0bBmSxMUXq4ifVIhkLYr9zGxG9NyAvaJpA9zdW5Wx7QbAkoTppUDHYuvsA2Bm7xAKDd7s7q8V35CZDQAGADRu3LiM3YpUUkuWwLHHhlbE+PFwyCFxRyR5IlmiaJah/TcFugANgfFmtr+7r0pcyd0HAYMA2rdv7xmISyR3TJkC7dpBo0bwyitw6KHh9FeRClJq15O7f5bskcK2lwGNEqYbRvMSLQVGuftGd18EzCckDhEpy/LlcPLJ0L59KAMO0K2bkoRUuFSuzN5Wk4CmZtbEzKoD/YBRxdYZSWhNYGb1CV1RC9MYk0juc4fHH4fmzUMZ8L/+VUX8JK1SuTJ7m7h7gZldAowmjD885u6zzexWYLK7j4qWHWVmc4BNwLVbOWAuUvn06xdKgR98MDzyCOy3X9wRSZ5LKVGYWS2gsbvP25qNu/srwCvF5v0p4bkDV0UPESlNYhG/Hj3COMRFF0GVdHYKiARl/pWZWS9gGvBaNN3azIp3IYlIunz0UbgN6aOPhumzzoJLLlGSkIxJ5S/tZsI1EasA3H0a4d4UIpJOGzeG8YcDDoA5c2CHHeKOSCqpVLqeNrr7d7blZf86RVUknaZNC1dUT5sWym7cfz/88pdxRyWVVCqJYraZnQZUjUpuXAa8m96wRCq55cvD47nn4MQT445GKrlUup4uJdwvewMwlFBu/Io0xiRSOU2YAA88EJ537w6ffKIkIVkhlUSxn7vf4O6/iR5/jGo/iUhFWL06DE4feijccw9s2BDmb799rGGJFEolUfzDzOaa2Z/NrGXaIxKpTEaPDkX8HngALr9cRfwkK5WZKNz9cMKd7b4GHjazmWb2x7RHJpLvliyBnj1Dy2HChNCa0JlNkoVSOhHb3Ze7+33AhYRrKv6U/BUiUiJ3+OCD8LxRI3j1VZg6VSU4JKulcsFdMzO72cxmAvcTznhqmPbIRPLNF1+E25B27FhUxO/II1XET7JeKqfHPgb8Bzja3T9Pczwi+ccdhgyBq66C9evhjjtCnSaRHFFmonD3TpkIRCRv9e0LI0aEs5oeeQT22SfuiES2SqmJwsyecfe+UZdT4pXYqd7hTqTy2rQpFPCrUgV69YKuXeGCC1SfSXJSshbF5dHPnpkIRCRvzJ0L554bSnCcfz6ceWbcEYmUS7I73H0RPb2ohLvbXZSZ8ERyyMaNcNtt0Lo1zJsHdevGHZFIhUilHdythHnHVHQgIjlt6tRwS9Ibb4QTTgitir59445KpEIkG6P4HaHl8Gszm5GwqA7wTroDE8kpX34J33wDI0dC795xRyNSoZKNUQwFXgVuB65PmL/a3b9Na1QiuWD8eJg5Ey6+OBTxW7AAatWKOyqRCpes68nd/VPgYmB1wgMz2yX9oYlkqe+/D7ch7dwZ7ruvqIifkoTkqbJaFD2BKYTTYxPvXOTAr9MYl0h2euWVcJrr55+HC+huvVVF/CTvlZoo3L1n9FO3PRWBUMSvd2/Yd99wAV3HjnFHJJIRqdR6OtjMakfPTzezu82scfpDE8kC7jBxYnjeqBG8/nooBa4kIZVIKqfHPgisNbMDgKuBT4An0xqVSDb4/HM4/njo1KmoiN/hh0P16rGGJZJpqSSKAnd3oDfwL3cfSDhFViQ/uYeaTM2bhxbEXXepiJ9UaqlUj11tZr8HzgAONbMqQLX0hiUSo5NOgv/+N5zV9MgjsPfecUckEqtUWhSnABuAc9x9OeFeFHemNSqRTNu0CTZvDs+PPx4eegjGjFGSECG1W6EuB54G6ppZT2C9uz+R9shEMmXWrNC19OijYfqMM1TpVSRBmV1PZtaX0IIYR7iW4n4zu9bdR6Q5thLNmwdduvx8/rRpoRabSMp+/BFuvx3+8pdQwG/nneOOSCQrpTJGcQPwG3f/CsDMdgXeAGJJFOvWlTy/dWs47bSMhiK5bMoU6N8/tCZOOw3uuQd23TXuqESyUiqJokphkoisILWxjbSoVQvGjYtr75I3VqyAVavgxRehp265IpJMKoniNTMbDQyLpk8BXklfSCJpMnZsKOJ32WVw1FHw8cdQs2bcUYlkvVQGs68FHgZaRY9B7n5dugMTqTDffRcGp7t2hQcfLCripyQhkpJk96NoCtwF7AXMBK5x92WZCkykQrz4Ilx4ISxfDtdcA7fcoiJ+IlspWYviMeAloA+hguz9GYlIpKIsWQJ9+kC9eqFe0513wvbbxx2VSM5JNkZRx90HR8/nmdmHmQhIpFzc4b334KCDior4HXSQ6jOJlEOyFkVNM2tjZm3NrC1Qq9h0mcysu5nNM7MFZnZ9kvX6mJmbWfutfQMiP1m6FI47Llw8V1jEr0sXJQmRckrWovgCuDthennCtANdk23YzKoCA4FuwFJgkpmNcvc5xdarA1wOvL91oYtENm+GwYPh2muhoADuvhsOOSTuqETyRrIbFx1ezm13ABa4+0IAMxtOqEA7p9h6fwbuAK4t5/6ksurTB0aODGc1DR4Mv9bNF0UqUjovnGsALEmYXhrN+0nUhdXI3V9OtiEzG2Bmk81s8saNGys+Usk9BQVFRfz69AkJ4o03lCRE0iC2K6yjcuV3E26GlJS7D3L39u7evlo1VTiv9GbMCDcTGhyda3H66XDeeWCW/HUisk3SmSiWAY0SphtG8wrVAVoC48zsU+BAYJQGtKVUGzbATTdBu3bw2WeqzSSSIancM9uie2X/KZpubGYdUtj2JKCpmTUxs+pAP2BU4UJ3/87d67v7nu6+JzAROM7dJ2/TO5H8NmkStG0Lt94Kp54Kc+fCiSfGHZVIpZBKi+IBoBNwajS9mnA2U1LuXgBcAowG5gLPuPtsM7vVzI7bxnilslq5EtasgVdegSeeCBfRiUhGWLgddpIVzD5097ZmNtXd20Tzprv7ARmJsJg6ddr76tVqdFQKY8aEIn6XXx6mN2xQ+Q2RbWRmU9x9m7r2U2lRbIyuifBoZ7sCm7dlZyIpWbUKzj8fjjgCHn64qIifkoRILFJJFPcBzwO/MLO/ABOAv6Y1Kqm8XngBmjeHxx6D//u/cIMhJQiRWJV5Pwp3f9rMpgBHEG6Fery7z017ZFL5LF4MJ58MzZrBqFHQXifAiWSDVO6Z3RhYC7yYOM/dF6czMKkk3GHCBDj0UGjcOFw0d+CBqs8kkkVSucPdy4TxCQNqAk2AeUCLNMYllcHixeFeEa++Gu5v27kzHHZY3FGJSDGpdD3tnzgdld24KG0RSf7bvBkeegiuuy60KO67T0X8RLJYKi2KLbj7h2bWMR3BSCVx4olh0LpbNxg0CPbcM+6IRCSJVMYorkqYrAK0BT5PW0SSnwoKoEqV8DjlFOjdG/r3V30mkRyQyumxdRIeNQhjFr3TGZTkmenToWPH0HqAUILj7LOVJERyRNIWRXShXR13vyZD8Ug+Wb8ebrsN7rgDdtkFfvnLuCMSkW1QaqIws+3cvcDMDs5kQJInPvgAzjoLPvoo/Lz77pAsRCTnJGtRfEAYj5hmZqOAZ4EfChe6+3/THJvksu+/h3Xr4LXX4Oij445GRMohlbOeagIrCPfILryewgElCtnS66/D7Nlw5ZVw5JEwb57Kb4jkgWSJ4hfRGU+zKEoQhZKXnJXKZeVKuOoqGDIEWrSAiy4KCUJJQiQvJDvrqSqwQ/Sok/C88CEC//1vKOL35JPw+9/D5MlKECJ5JlmL4gt3vzVjkUjuWbwY+vWDli3DDYXatIk7IhFJg2QtCp3kLj/nDm+9FZ43bhxuLvT++0oSInksWaI4ImNRSG747DM45hjo0qUoWRxyCFSrFmtYIpJepSYKd/82k4FIFtu8Gf71rzBQPWEC3H9/KAsuIpXCVhcFlEro+OPhxRfD9RAPPwx77BF3RCKSQUoUUrKNG6Fq1VDE79RT4aST4IwzVJ9JpBJKpSigVDYffggdOoR7RkBIFGeeqSQhUkkpUUiRdevCtRAdOsDy5dCoUdwRiUgWUNeTBBMnhuJ98+fDOefAXXfBzjvHHZWIZAElCgl++CGMS/zvf6FOk4hIRImiMnvttVDE7+qr4YgjQknw6tXjjkpEsozGKCqjFStCN9Mxx8Djj8OPP4b5ShIiUgIlisrEHUaMCEX8hg6FP/4RJk1SghCRpNT1VJksXgynnQatWoV7RxxwQNwRiUgOUIsi37mHwn0QrqgeNy6c4aQkISIpUqLIZ4sWwVFHhYHqwiJ+Bx0E26khKSKpU6LIR5s2wb33hvtEvP8+PPigiviJyDbTV8t81Ls3vPwy9OgRynDoCmsRKQclinyRWMTvjDNCfabTTlN9JhEpt7R2PZlZdzObZ2YLzOz6EpZfZWZzzGyGmb1pZqpfvS0mT4b27UMXE8App8Bvf6skISIVIm2JwsyqAgOBY4DmwKlm1rzYalOB9u7eChgB/D1d8eSldevguuugY0f4+mvdJ0JE0iKdLYoOwAJ3X+juPwLDgd6JK7j7WHdfG01OBBqmMZ788t574RTXv/89FPGbMwd69ow7KhHJQ+kco2gALEmYXgp0TLL+ucCrJS0wswHAAIAaNVpVVHy5bd26cIvSN94Ip7+KiKRJVgxmm9npQHugc0nL3X0QMAigTp32nsHQsssrr4QiftdeC127wty5UK1a3FGJSJ5LZ9fTMiDxvMyG0bwtmNmRwA3Ace6+IY3x5K5vvoHTT4djj4Wnny4q4qckISIZkM5EMQloamZNzKw60A8YlbiCmbUBHiYkia/SGEtucofhw6FZM3jmGbjpJvjgAxXxE5GMSlvXk7sXmNklwGigKvCYu882s1uBye4+CrgT2AF41sKpnIvd/bh0xZRzFi8O5cAPOAAefRT23z/uiESkEjL33Oryr1Onva9ePTnuMNLHHd58s+gucxMnwm9+Ey6mExHZRmY2xd3bb8trVespm3zySTiDqVu3oiJ+Bx6oJCEisVKiyAabNsHdd4eupSlT4OGHVcRPRLJGVpweW+n16gWvvhoumHvwQWio6w5FJHsoUcTlxx/DfSGqVIH+/UMhv379VJ9JRLKOup7i8MEH0K4dPPBAmO7bN1R7VZIQkSykRJFJa9fC1VdDp06wciXstVfcEYmIlEldT5kyYUK4JmLhQrjgArjjDqhbN+6oRETKpESRKYU3Fho7Frp0iTsaEZGUKVGk04svhsJ9//d/cPjhoRT4djrkIpJbNEaRDl9/HW5DetxxMGxYURE/JQkRyUFKFBXJHYYODUX8RoyAW2+F999XET8RyWn6iluRFi+Gs8+GNm1CEb8WLeKOSESk3NSiKK/Nm2H06PB8jz3g7bfhnXeUJEQkbyhRlMfHH4c7zXXvDuPHh3kdOqiIn4jkFSWKbVFQAHfeCa1awbRpoZtJRfxEJE9pjGJb9OwZupt69w5lOH71q7gjEslKGzduZOnSpaxfvz7uUCqNmjVr0rBhQ6pV4K2SdeOiVG3YEO5RXaVKOKNp82Y4+WTVZxJJYtGiRdSpU4d69eph+l9JO3dnxYoVrF69miZNmmyxTDcuSreJE6FtWxg4MEyfdFIo5Kc/fJGk1q9frySRQWZGvXr1KrwFp0SRzA8/wJVXwkEHwerV0LRp3BGJ5BwlicxKx/HWGEVp3n47FPFbtAguughuvx123DHuqEREMk4titIUFIQxibfeCl1OShIiOWvkyJGYGR999NFP88aNG0fPnj23WK9///6MGDECCAPx119/PU2bNqVt27Z06tSJV199tdyx3H777ey9997su+++jC68BquYMWPG0LZtW1q2bMlZZ51FQUHBFnG3bt2aFi1a0Llz53LHkwolikQjR4aWA4QifrNnw2GHxRqSiJTfsGHDOOSQQxg2bFjKr7nxxhv54osvmDVrFh9++CEjR45k9erV5Ypjzpw5DB8+nNmzZ/Paa69x0UUXsWnTpi3W2bx5M2eddRbDhw9n1qxZ7LHHHjz++OMArFq1iosuuohRo0Yxe/Zsnn322XLFkyp1PQF8+SVceik8+2wYtL766lCfSUX8RCrMFVeEy44qUuvWcM89yddZs2YNEyZMYOzYsfTq1YtbbrmlzO2uXbuWwYMHs2jRImrUqAHAbrvtRt++fcsV7wsvvEC/fv2oUaMGTZo0Ye+99+aDDz6gU6dOP62zYsUKqlevzj777ANAt27duP322zn33HMZOnQoJ554Io0bNwbgF7/4RbniSVXlblG4w5NPQvPm8MIL8Je/hDOcVMRPJG+88MILdO/enX322Yd69eoxZcqUMl+zYMECGjduzI4pdDlfeeWVtG7d+mePv/3tbz9bd9myZTRq1Oin6YYNG7Js2bIt1qlfvz4FBQVMnhwuAxgxYgRLliwBYP78+axcuZIuXbrQrl07nnjiiTLjqwiV+yvz4sVw3nnQvn24unq//eKOSCRvlfXNP12GDRvG5ZdfDkC/fv0YNmwY7dq1K/XsoK09a+if//xnuWMsvv/hw4dz5ZVXsmHDBo466iiqRmWBCgoKmDJlCm+++Sbr1q2jU6dOHHjggT+1PtKl8iWKwiJ+xxwTivi9806o9qr6TCJ559tvv2XMmDHMnDkTM2PTpk2YGXfeeSf16tVj5cqVP1u/fv367L333ixevJjvv/++zFbFlVdeydixY382v1+/flx//fVbzGvQoMFPrQOApUuX0qBBg5+9tlOnTrz99tsAvP7668yfPx8ILZB69epRu3ZtateuzWGHHcb06dPTnihw95x67LBDO99m8+a5H3qoO7iPG7ft2xGRlMyZMyfW/T/88MM+YMCALeYddthh/tZbb/n69et9zz33/CnGTz/91Bs3buyrVq1yd/drr73W+/fv7xs2bHB396+++sqfeeaZcsUza9Ysb9Wqla9fv94XLlzoTZo08YKCgp+t9+WXX7q7+/r1671r167+5ptvuns4nl27dvWNGzf6Dz/84C1atPCZM2f+7PUlHXdgsm/j527lGKMoKIA77ghF/GbOhH//W2cziVQCw4YN44QTTthiXp8+fRg2bBg1atTgqaee4uyzz6Z169acdNJJPPLII9StWxeA2267jV133ZXmzZvTsmVLevbsmdKYRTItWrSgb9++NG/enO7duzNw4MCfupV69OjB559/DsCdd95Js2bNaNWqFb169aJr164ANGvWjO7du9OqVSs6dOjAeeedR8uWLcsVUyoqR62no4+G11+HE08M10T88pfpCU5EtjB37lyaNWsWdxiVTknHvTy1nvJ3jGL9+nDBXNWqMGBAePTpE3dUIiI5Jz+7nt55J5xgXVjEr08fJQkRkW2UX4lizRq47LJwE6H160FNXpHY5Vr3dq5Lx/HOn0Tx1lvQsiX8619wySUwaxZ06xZ3VCKVWs2aNVmxYoWSRYZ4dD+KmjVrVuh282uMYvvtQ9XXgw+OOxIRIZz3v3TpUr7++uu4Q6k0Cu9wV5Fy+6yn//4XPvoI/vCHML1pky6cExEpQdbe4c7MupvZPDNbYGbXl7C8hpn9J1r+vpntmdKGly8Pd5nr0weefx5+/DHMV5IQEalwaUsUZlYVGAgcAzQHTjWz5sVWOxdY6e57A/8E7ihru3U3rgiD1C+9FEqCv/uuiviJiKRROlsUHYAF7r7Q3X8EhgO9i63TG3g8ej4COMLKqMi124bPwqD19Olw/fXhWgkREUmbdA5mNwCWJEwvBTqWto67F5jZd0A94JvElcxsADAgmtxgEybMUqVXAOpT7FhVYjoWRXQsiuhYFNl3W1+YE2c9ufsgYBCAmU3e1gGZfKNjUUTHooiORREdiyJmtpW1j4qks+tpGdAoYbphNK/EdcxsO6AusCKNMYmIyFZKZ6KYBDQ1syZmVh3oB4wqts4o4Kzo+UnAGM+183VFRPJc2rqeojGHS4DRQFXgMXefbWa3EuqijwIeBZ40swXAt4RkUpZB6Yo5B+lYFNGxKKJjUUTHosg2H4ucu+BOREQyK39qPYmISFooUYiISFJZmyjSVv4jB6VwLK4yszlmNsPM3jSzPeKIMxPKOhYJ6/UxMzezvD01MpVjYWZ9o7+N2WY2NNMxZkoK/yONzWysmU2N/k96xBFnupnZY2b2lZnNKmW5mdl90XGaYWZtU9rwtt5sO50PwuD3J8CvgerAdKB5sXUuAh6KnvcD/hN33DEei8OB7aPnv6vMxyJarw4wHpgItI877hj/LpoCU4Gdo+lfxB13jMdiEPC76Hlz4NO4407TsTgMaAvMKmV5D+BVwIADgfdT2W62tijSUv4jR5V5LNx9rLuvjSYnEq5ZyUep/F0A/JlQN2x9JoPLsFSOxfnAQHdfCeDuX2U4xkxJ5Vg4sGP0vC7weQbjyxh3H084g7Q0vYEnPJgI7GRmu5e13WxNFCWV/2hQ2jruXgAUlv/IN6kci0TnEr4x5KMyj0XUlG7k7i9nMrAYpPJ3sQ+wj5m9Y2YTzax7xqLLrFSOxc3A6Wa2FHgFuDQzoWWdrf08AXKkhIekxsxOB9oDneOOJQ5mVgW4G+gfcyjZYjtC91MXQitzvJnt7+6r4gwqJqcCQ9z9H2bWiXD9Vkt33xx3YLkgW1sUKv9RJJVjgZkdCdwAHOfuGzIUW6aVdSzqAC2BcWb2KaEPdlSeDmin8nexFBjl7hvdfREwn5A48k0qx+Jc4BkAd38PqEkoGFjZpPR5Uly2JgqV/yhS5rEwszbAw4Qkka/90FDGsXD379y9vrvv6e57EsZrjnP3bS6GlsVS+R8ZSWhNYGb1CV1RCzMYY6akciwWA0cAmFkzQqKojPdnHQWcGZ39dCDwnbt/UdaLsrLrydNX/iPnpHgs7gR2AJ6NxvMXu/txsQWdJikei0ohxWMxGjjKzOYAm4Br3T3vWt0pHourgcFmdiVhYLt/Pn6xNLNhhC8H9aPxmJuAagDu/hBhfKYHsABYC5yd0nbz8FiJiEgFytauJxERyRJKFCIikpQShYiIJKVEISIiSSlRiIhIUkoUkpXMbJOZTUt47Jlk3TUVsL8hZrYo2teH0dW7W7uNR8ysefT8D8WWvVveGKPtFB6XWWb2opntVMb6rfO1Uqpkjk6PlaxkZmvcfYeKXjfJNoYAL7n7CDM7CrjL3VuVY3vljqms7ZrZ48B8d/9LkvX7EyroXlLRsUjloRaF5AQz2yG618aHZjbTzH5WNdbMdjez8QnfuA+N5h9lZu9Fr33WzMr6AB8P7B299qpoW7PM7IpoXm0ze9nMpkfzT4nmjzOz9mb2N6BWFMfT0bI10c/hZnZsQsxDzOwkM6tqZnea2aToPgEXpHBY3iMq6GZmHaL3ONXM3jWzfaOrlG8FToliOSWK/TEz+yBat6TquyJbirt+uh56lPQgXEk8LXo8T6gisGO0rD7hytLCFvGa6OfVwA3R86qE2k/1CR/8taP51wF/KmF/Q4CToucnA+8D7YCZQG3Cle+zgTZAH2BwwmvrRj/HEd3/ojCmhHUKYzwBeDx6Xp1QybMWMAD4YzS/BjAZaFJCnGsS3t+zQPdoekdgu+j5kcBz0fP+wL8SXv9X4PTo+U6E+k+14/5965Hdj6ws4SECrHP31oUTZlYN+KuZHQZsJnyT3g1YnvCaScBj0boj3X2amXUm3Kjmnai8SXXCN/GS3GlmfyTUADqXUBvoeXf/IYrhv8ChwGvAP8zsDkJ31dtb8b5eBe41sxpAd2C8u6+LurtamdlJ0Xp1CQX8FhV7fS0zmxa9/7nA/xLWf9zMmhJKVFQrZf9HAceZ2TXRdE2gcbQtkRIpUUiu+C2wK9DO3TdaqA5bM3EFdx8fJZJjgSFmdjewEvifu5+awj6udfcRhRNmdkRJK7n7fAv3vegB3GZmb7r7ram8CXdfb2bjgKOBUwg32YFwx7FL3X10GZtY5+6tzWx7Qm2ji4H7CDdrGuvuJ0QD/+NKeb0Bfdx9XirxioDGKCR31AW+ipLE4cDP7gtu4V7hX7r7YOARwi0hJwIHm1nhmENtM9snxX2+DRxvZtubWW1Ct9HbZvYrYK27P0UoyFjSfYc3Ri2bkvyHUIytsHUC4UP/d4WvMbN9on2WyMMdDS8DrraiMvuF5aL7J6y6mtAFV2g0cKlFzSsLlYdFklKikFzxNNDezGYCZwIflbBOF2C6mU0lfFu/192/JnxwDjOzGYRup/1S2aG7f0gYu/iAMGbxiLtPBfYHPoi6gG4Cbivh5YOAGYWD2cW8Tri51Bsebt0JIbHNAT40s1mEsvFJW/xRLDMIN+X5O3B79N4TXzcWaF44mE1oeVSLYpsdTYskpdNjRUQkKbUoREQkKSUKERFJSolCRESSUqIQEZGklChERCQpJQoREUlKiUJERJL6f3MiAu+xdRM1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(BertBilstmClassifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f735774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
