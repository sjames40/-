{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "#import fasttext\n",
    "#import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edeaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb71cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052074671668017\n",
      "0.7045793841375075\n",
      "0.8142993898625756\n",
      "0.7122102717678036\n"
     ]
    }
   ],
   "source": [
    "text = SnowNLP(u'口感很好，喝起来味道不错，包装也很精美，送人也很大气。')\n",
    "sent = text.sentences\n",
    "for sen in sent:\n",
    "    s = SnowNLP(sen)\n",
    "    print(s.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['送', '人', '也', '很', '大', '气']\n"
     ]
    }
   ],
   "source": [
    "print(s.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3192e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('送', 'v'), ('人', 'n'), ('也', 'd'), ('很', 'd'), ('大', 'a'), ('气', 'n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab6a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e398a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"teapro.csv\", encoding=\"GBK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc8ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        no                  rateContent  package  quality  price  service  \\\n",
      "0        1  口感很好，喝起来味道不错，包装也很精美，送人也很大气。        1        1      0        0   \n",
      "1        2  送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！        1        0      0        0   \n",
      "2        3            茶的味道很纯正，使用方便，包装很好        1        1      0        0   \n",
      "3        4              茶叶不错，味道挺好的，5分好评        0        1      0        0   \n",
      "4        5                     口感特别好~~！        0        1      0        0   \n",
      "...    ...                          ...      ...      ...    ...      ...   \n",
      "3842  3843                很好，很新鲜，不错，好评！        0        0      0        0   \n",
      "3843  3844               不错，是正品，下次还会再来。        0        1      0        0   \n",
      "3844  3845      老板态度好，发货及时，茶叶很好，口感很好，甘甜        0        1      0        1   \n",
      "3845  3846                     哎，，，，，，，        0        0      0        0   \n",
      "3846  3847                       味道真的不错        0        1      0        0   \n",
      "\n",
      "      logistics  other  sentiment  \n",
      "0             0      0          0  \n",
      "1             0      0          1  \n",
      "2             0      0          0  \n",
      "3             0      0          0  \n",
      "4             0      0          0  \n",
      "...         ...    ...        ...  \n",
      "3842          0      0          0  \n",
      "3843          0      0          0  \n",
      "3844          1      0          0  \n",
      "3845          0      0          1  \n",
      "3846          0      0          0  \n",
      "\n",
      "[3847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcfb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c98ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n",
      "1       送朋友的，如果里盒不破就更好了，细节也很重要哦，谢谢！\n",
      "2                 茶的味道很纯正，使用方便，包装很好\n",
      "3                   茶叶不错，味道挺好的，5分好评\n",
      "4                          口感特别好~~！\n",
      "                   ...             \n",
      "3842                  很好，很新鲜，不错，好评！\n",
      "3843                 不错，是正品，下次还会再来。\n",
      "3844        老板态度好，发货及时，茶叶很好，口感很好，甘甜\n",
      "3845                       哎，，，，，，，\n",
      "3846                         味道真的不错\n",
      "Name: rateContent, Length: 3847, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[:,\"rateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2fa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "口感很好，喝起来味道不错，包装也很精美，送人也很大气。\n"
     ]
    }
   ],
   "source": [
    "training_data_list = []\n",
    "print(train.loc[:,\"rateContent\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc74bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[:,\"rateContent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f016d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    training_data_list.append(train.loc[:,\"rateContent\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e2a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_data_list.append(train.loc[:,\"rateContent\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in range(len(train.loc[:,\"rateContent\"])-100):\n",
    "    label_list.append(train.loc[:,\"package\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7598d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_label_list = []\n",
    "for j in range(len(train.loc[:,\"rateContent\"])-100,len(train.loc[:,\"rateContent\"])):\n",
    "    vali_label_list.append(train.loc[:,\"package\"][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40ddd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=64,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36865e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba976637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(training_data_list)\n",
    "val_inputs, val_masks = preprocessing_for_bert(vali_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b9aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(label_list)\n",
    "val_labels = torch.tensor(vali_label_list)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa47ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 300\n",
    "    hidden_size = 10\n",
    "    output_size = 4\n",
    "    max_epochs = 30\n",
    "    lr = 0.5\n",
    "    batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e6ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1273c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fastText(nn.Module):\n",
    "    def __init__(self, config, vocab_size, word_embeddings):\n",
    "        super(fastText, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        #self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        #self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.fc1 = nn.Linear(self.config.embed_size, self.config.hidden_size)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc2 = nn.Linear(self.config.hidden_size, self.config.output_size)\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #embedded_sent = self.embeddings(x).permute(1,0,2)\n",
    "        texts = self.bert(x)[0].detach_()\n",
    "\n",
    "        texts = texts.permute(0, 2, 1)\n",
    "        #if bool(self.dropout[0]):\n",
    "        #    texts = self.drp1(texts) \n",
    "        h = self.fc1(texts.mean(1))\n",
    "        z = self.fc2(h)\n",
    "        return self.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "873176a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    fastText = fastText(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    fastText.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe2c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([3, 64])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "  for step, batch in enumerate(train_dataloader):\n",
    "            #batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t for t in batch)\n",
    "            print(b_input_ids.shape)\n",
    "            print(b_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bb74ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        #model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            #print(b_labels.dtype)\n",
    "            #print(b_labels)\n",
    "            #print(b_labels.shape)\n",
    "            #b_labels = b_labels.float32\n",
    "            #b_labels = torch.tensor(b_labels,dtype=torch.float32)\n",
    "            \n",
    "            # Zero out any previously calculated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids)\n",
    "            #logits = torch.tensor(logits,dtype=torch.float32)\n",
    "            #print(logits.shape)\n",
    "            #print(logits.dtype)\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        #b_label = torch.float(b)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b2cc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9f02741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 64\n",
    "    hidden_size = 368\n",
    "    output_size = 4\n",
    "    max_epochs = 30\n",
    "    lr = 0.5\n",
    "    batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baea4480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/shijunliang/Desktop/enter/envs/transform/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67987/2328506674.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   20    |   1.394686   |     -      |     -     |   1.01   \n",
      "   1    |   40    |   1.384746   |     -      |     -     |   0.55   \n",
      "   1    |   60    |   1.375274   |     -      |     -     |   0.55   \n",
      "   1    |   80    |   1.365365   |     -      |     -     |   0.55   \n",
      "   1    |   100   |   1.355856   |     -      |     -     |   0.55   \n",
      "   1    |   117   |   1.348251   |     -      |     -     |   0.46   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   1.371470   |  1.339903  |   92.97   |   3.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   1.339395   |     -      |     -     |   0.59   \n",
      "   2    |   40    |   1.330119   |     -      |     -     |   0.58   \n",
      "   2    |   60    |   1.319632   |     -      |     -     |   0.56   \n",
      "   2    |   80    |   1.311607   |     -      |     -     |   0.56   \n",
      "   2    |   100   |   1.305682   |     -      |     -     |   0.56   \n",
      "   2    |   117   |   1.300133   |     -      |     -     |   0.48   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   1.318393   |  1.285669  |   92.97   |   3.42   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "#bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "#params.update(learning_params)\n",
    "#params.update(cnn_params)\n",
    "#net = FastText(params)\n",
    "config = Config()\n",
    "net = fastText(config, 3500, 768)\n",
    "net = net.to(device)\n",
    "optimizer = AdamW(net.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "total_steps = len(train_dataloader) *4\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "train(net, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "642aa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "             logits = model(b_input_ids)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e12ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bebe0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f69abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6545\n",
      "Accuracy: 91.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67987/2328506674.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(z)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(net, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9d16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
